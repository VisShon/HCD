[
    {
        "profInfo": {
            "image": "amanparnami.png",
            "name": "Dr. Aman Parnami",
            "archive" : true
        },
        "projects": [
            {
                "title": "Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching",
                "DOI": "https://doi.org/10.1145/3544548.3581449",
                "abstract": "This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.",
                "year": "2023-24"
            },
            {
                "title": "MenstruWear: In-The-Wild Study For Characterizing The Effect Of User Activities On Sanitary Napkin-Based Bodily Fluid Monitoring",
                "DOI": "https://doi.org/10.1145/3605390.3605404",
                "abstract": "Wearable sanitary napkin sensors can capture the effect of physiological variations and activities of users in a real-life setting, while lab studies tend to focus on input parameter changes. We introduce a wearable sanitary napkin sensor system to explore discharge bodily fluid volume measurement opportunities in the wild. We use this system to understand the impact of body parameters and activities on the sensing system, such as standing, sitting, walking, and lying down. We identified several engineering, methodological, and ethical challenges associated with wearable intimate device design. Our discussions contribute to an ongoing dialogue around women’s health in HCI and the barriers to personal care technology design. Through four-hour-long user studies with 10 participants on different activities, we showed that the sensing system behaves uniformly for all activities and across users. Using feedback interviews with participants, we find that our system is comfortable for performing activities and valuable technology for females.",
                "year": "2023-24"
            },
            {
                "title": "Enabling rapid prototyping of tangible augmented reality experiences",
                "DOI": "https://repository.iiitd.edu.in/xmlui/handle/123456789/1357",
                "abstract": "We introduce Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.",
                "year": "2023-24"
            },
            {
                "title": "AttentioNet: Monitoring Student Attention Type in Learning with EEG-Based Measurement System.",
                "DOI": "https://doi.org/10.1109/ACII59096.2023.10388212",
                "abstract": "Student attention is an indispensable input for uncovering their goals, intentions, and interests, which prove to be invaluable for a multitude of research areas, ranging from psychology to interactive systems. However, most existing methods to classify attention fail to model its complex nature. To bridge this gap, we propose AttentioNet, a novel Convolutional Neural Network-based approach that utilizes Electroencephalography (EEG) data to classify attention into five states: Selective, Sustained, Divided, Alternating, and relaxed state. We collected a dataset of 20 subjects through standard neuropsychological tasks to elicit different attentional states. The average across-student accuracy of our proposed model at this configuration is 92.3% (SD=3.04), which is well-suited for end-user applications. Our transfer learning-based approach for personalizing the model to individual subjects effectively addresses the issue of individual variability in EEG signals, resulting in improved performance and adaptability of the model for real-world applications. This represents a significant advancement in the field of EEG-based classification. Experimental results demonstrate that AttentioNet outperforms a popular EEGnet baseline (p-value < 0.05) in both subject-independent and subject-dependent settings, confirming the effectiveness of our proposed approach despite the limitations of our dataset. These results highlight the promising potential of AttentioNet for attention classification using EEG data.",
                "year": "2023-24"
            },
            {
                "title": "Investigating Spatial Representation of Learning Content in Virtual Reality Learning Environments",
                "DOI": "https://doi.org/10.1109/VR55154.2023.00019",
                "abstract": "A recent surge in the application of Virtual Reality in education has made VR Learning Environments (VRLEs) prevalent in fields ranging from aviation, medicine, and skill training to teaching factual and conceptual content. In spite of multiple 3D affordances provided by VR, learning content placement in VRLEs has been mostly limited to a static placement in the environment. We conduct two studies to investigate the effect of different spatial representations of learning content in virtual environments on learning outcomes and user experience. In the first study, we studied the effects of placing content at four different places - world-anchored (TV screen placed in the environment), user-anchored (panel anchored to the wrist or head-mounted display of the user) and object-anchored (panel anchored to the object associated with current content) - in the VR environment with forty-two participants in the context of learning how to operate a laser cutting machine through an immersive tutorial. In the follow-up study, twenty-two participants from this study were given the option to choose from these four placements to understand their preferences. The effects of placements were examined on learning outcome measures - knowledge gain, knowledge transfer, cognitive load, user experience, and user preferences. We found that participants preferred user-anchored (controller condition) and object-anchored placement. While knowledge gain, knowledge transfer, and cognitive load were not found to be significantly different between the four conditions, the object-anchored placement scored significantly better than the TV screen and head-mounted display conditions on the user experience scales of attractiveness, stimulation, and novelty.",
                "year": "2023-24"
            },
            {
                "title": "Akash Chaudhary, Manshul Belani, Naman Maheshwari, and Aman Parnami. 2021. Verbose : Designing a Context-based Educational System for Improving Communicative Expressions. In Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI '21). Association for Computing Machinery, New York, NY, USA, Article 41, 1–13. ",
                "DOI": "https://doi.org/10.1145/3447526.3472057",
                "abstract": "ESL (English as a second language) speakers tend to follow the tone structure of their first language, making their speech difficult to understand for native speakers, thereby limiting their opportunities for education and employment. To address this problem, we build an interactive smartphone-based educational mobile application using the user-centered design process. This application teaches English intonations based on globally consistent pitch patterns through conversations with a trained chat assistant, which inculcates expert linguists’ teaching principles. After co-designing the application’s parameters with primary stakeholders and expert visual designers, we assess its effectiveness by measuring the pre and post-performance of the users after the system usage, using various quantitative measures, like intonation scores, SEQ, and SUS. Feedback from users suggests that ESL speakers find significant improvement in the perception of their vocal expressions, thereby highlighting the necessity of such a system in improving the quality of conversations that people have in general."
            },
            {
                "title": "Dhruv Verma, Sejal Bhalla, Dhruv Sahnan, Jainendra Shukla, and Aman Parnami. 2021. ExpressEar: Sensing Fine-Grained Facial Expressions with Earables. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 3, Article 129 (Sept 2021), 28 pages.",
                "DOI": "https://doi.org/10.1145/3478085",
                "abstract": "Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar's applicability in the real world and open up research opportunities to advance its practical adoption."
            },
            {
                "title": "Arpit Bhatia, Dhruv Kundu, Suyash Agarwal, Varnika Kairon, and Aman Parnami. 2021. Soma-noti: Delivering Notifications Through Under-clothing Wearables. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 8 pages.",
                "DOI": "https://doi.org/10.1145/3411764.3445123",
                "abstract": "Different form factors of wearable technology provide unique opportunities for output based on how they are connected to the human body. In this work, we investigate the idea of delivering notifications through devices worn on the underside of a user’s clothing. A wearable worn in such a manner is in direct contact with the user’s skin. We leverage this proximity to test the performance of 10 on-skin sensations (Press, Poke, Pinch, Heat, Cool, Blow, Suck, Vibrate, Moisture and Brush) as methods of notification delivery. We developed prototypes for each stimulus and conducted a user study to evaluate them across 6 locations commonly covered by upper body clothing. Results indicate significant differences in reaction time, error rates and comfort which may influence the design of future under-clothing wearables."
            }
        ]
    },
    {
        "profInfo": {
            "image": "anmols.jpg",
            "name": "Dr. Anmol Srivastava",
            "archive" : false
        },
        "projects": [
            {
                "title": "Understanding HCI Approaches for the Metaverse in Education Applications for the Global South",
                "DOI": "https://doi.org/10.1007/978-3-031-42293-5_93",
                "abstract": "The recent adoption of the Metaverse in various sectors indicates its potential for digital transformation. Technologies like XR, where X = Augmented/ Virtual/ Mixed Reality, will be its key enablers and can be powerful tools for developing nations’ digital educational transformation. These developing nations are often categorized under the Global South. This workshop presents a first step toward the socio-technical Human-Computer Interaction (HCI) aspects of the Metaverse to understand its impact on education in the Global South. The socio-technical approach helps cover human, social, and organizational factors, leading to more acceptable systems for end users and stakeholders. With the Metaverse, students, and teachers can log in via different immersive devices and experience different virtual environments fabricated to their individual needs, learning, and teaching styles. It will open up possibilities to explore new situations and access facilities that might be impossible due to physical world constraints. Two key concepts will be covered – (i) A socio-technical HCI approach to the Metaverse and (ii) an Interaction Design perspective on Avatar representation and understanding of human work in the Metaverse. This workshop will initiate dialogs on questions: (a) What are the socio-technical issues of the Metaverse for education in the global south? (b) Are there any cross-cultural usability and interaction design concerns regarding digital avatar representation? (c) What considerations will be required for educational Metaverse human-work interaction design?",
                "year": "2023-24"
            },
            {
                "title": "Safar: Heuristics for Augmented Reality Integration in Cultural Heritage",
                "DOI": "https://doi.org/10.1145/3626485.3626538",
                "abstract": "This research explores the integration of Augmented Reality (AR) technology to enhance historical site exploration, with a particular focus on the Indian context. The primary objective is to establish comprehensive design guidelines that seamlessly blend AR and cultural heritage, ultimately enriching the heritage tourism experience. Through a case study approach centred around a prominent historical site, the research utilises co-creation sessions, alongside thorough primary and secondary research practices, to analyse user interactions with AR and Virtual Reality (VR) tools. While relying primarily on qualitative insights, this study uncovers the potential of AR in heightening the heritage encounter and bridging the gap between traditional narratives and contemporary technology. By distilling findings from these methodologies, the paper contributes practical and informed design guidelines for effectively integrating AR within cultural sites. The outcomes of this research provide valuable insights for scholars, practitioners, and enthusiasts seeking to navigate the evolving landscape of technology-driven heritage tourism.",
                "year": "2023-24"
            },
            {
                "title": "A Rapid Scoping Review and Conceptual Analysis of the Educational Metaverse in the Global South: Socio-Technical Perspectives",
                "DOI": "https://doi.org/10.48550/arXiv.2401.00338",
                "abstract": "This paper presents a conceptual insight into the Design of the Metaverse to facilitate educational transformation in selected developing nations within the Global South regions, e.g., India. These regions are often afflicted with socio-economic challenges but rich in cultural diversity. By utilizing a socio-technical design approach, this study explores the specific needs and opportunities presented by these diverse settings. A rapid scoping review of the scant existing literature is conducted to provide fundamental insights. A novel design methodology was formulated that utilized ChatGPT for ideation, brainstorming, and literature survey query generation. This paper aims not only to shed light on the educational possibilities enabled by the Metaverse but also to highlight design considerations unique to the Global South.",
                "year": "2023-24"
            },
            {
                "title": "KaavadBits: Exploring Tangible Interactive Storytelling of Branching Narratives through a Kaavad-inspired Installation",
                "DOI": "https://doi.org/10.1145/3623509.3635251",
                "abstract": "This work explores branching narratives through KaavadBits, a tabletop art installation embodying the kaavadiya-jajmaan or the narrator-patron perspective of kaavad baanchana, the Indian storytelling tradition of reciting the kaavad. For diegetic worldbuilding of tales from Panchatantra, a compilation of ancient Indian animal fables, the narrator takes the physical form of a tree, with which the audience interacts using tokens for a seamless multi-modal storytelling experience. Building on the related explorations, we propose a novel design that immerses the audience through choice, character and question-based interactions. We discuss the insights from a pilot user study and directions for future work. Through this paper, we aim to strike consequential tangible, technological and narrative explorations into various lesser-known traditional forms of storytelling that may inspire new interaction techniques, ultimately preserving the intangible heritages.",
                "year": "2023-24"
            }
        ]
    },
    {
        "profInfo": {
            "image": "Jainendra.png",
            "name": "Dr. Jainendra Shukla",
            "archive" : false
        },
        "projects": [
            {
                "title": "Opacity, Transparency, and the Ethics of Affective Computing",
                "DOI": "https://doi.org/10.1109/TAFFC.2023.3278230",
                "abstract": "Human opacity is the intrinsic quality of unknowability of human beings with respect to machines. The descriptive relationship between humans and machines, which captures how much information one can gather about the other, can be explicated using an opacity-transparency relationship. This relationship allows us to describe and normatively evaluate a spectrum of opacity where humans and machines may be either opaque or transparent. In this paper, we argue that the advent of Affective Computing (AC) has begun to shift the ideal position of humans on this spectrum towards greater transparency, while much of this technology is shifting towards opacity. We explore the implications of this shift with regard to the affective information of humans and how the threat to human opacity by AC systems has various adverse repercussions, such as infringement of one's autonomy, deception, manipulation, and increased anxiety. There are also distributive consequences that expose vulnerable groups to unjustified burdens and reduce them to mere profiles. We further provide an assessment of current AC technology, which follows the descriptive relationship between humans and machines from the lens of opacity and transparency. Finally, we foresee and address three possible objections to our claims. These are the beneficence of AC systems, their relation to privacy, and their restrictive capacity to capture human affects. Through these arguments, the paper aims to bring attention to the ontological relationship between humans and machines from the perspective of opacity and transparency while emphasizing on the gravity of the ethical concerns raised by their threat to human opacity.",
                "year": "2023-24"
            },
            {
                "title": "SPASHT: Semantic and Pragmatic Speech Features for Automatic Assessment of Autism",
                "DOI": "https://doi.org/10.1109/ICASSP49357.2023.10095135",
                "abstract": "Language and communication impairments are considered one of the core features of autism spectrum disorder (ASD). Quantifying the language atypicalities in autism is a challenging task. Prior works have explored acoustic, and text-based features to assess children’s language and communicative behaviours and have shown their relevance in the diagnosis of autism. In this work, we explore the semantic and pragmatic language features in children with autism (CwA) to understand their significance in the diagnosis of autism. We use natural language processing (NLP) and machine learning (ML) techniques to automatically extract relevant features and detect the existence of speech behaviours such as echolalia, semantic coherence, repetitive language, etc. We further analyse their correlation with the clinical diagnosis of autism. We conducted validation experiments on the transcripts of 76 children (35 ASD and 41 TD) extracted from the CHILDES databank. Our analysis shows that the semantic and pragmatic language features are representative candidates for autism diagnosis and are found to complement the syntactic and lexical features in the classification of CwA with an accuracy of 94%. Further, these features being more coherent and relatable to the standard diagnostic tools improves the interpretability of the diagnostic predictions made using speech signals.",
                "year": "2023-24"
            },
            {
                "title": "An Analysis of Physiological and Psychological Responses in Virtual Reality and Flat Screen Gaming",
                "DOI": "https://doi.org/10.48550/arXiv.2306.09690",
                "abstract": "Recent research has focused on the effectiveness of Virtual Reality (VR) in games as a more immersive method of interaction. However, there is a lack of robust analysis of the physiological effects between VR and flatscreen (FS) gaming. This paper introduces the first systematic comparison and analysis of emotional and physiological responses to commercially available games in VR and FS environments. To elicit these responses, we first selected four games through a pilot study of 6 participants to cover all four quadrants of the valence-arousal space. Using these games, we recorded the physiological activity, including Blood Volume Pulse and Electrodermal Activity, and self-reported emotions of 33 participants in a user study. Our data analysis revealed that VR gaming elicited more pronounced emotions, higher arousal, increased cognitive load and stress, and lower dominance than FS gaming. The Virtual Reality and Flat Screen (VRFS) dataset, containing over 15 hours of multimodal data comparing FS and VR gaming across different games, is also made publicly available for research purposes. Our analysis provides valuable insights for further investigations into the physiological and emotional effects of VR and FS gaming.",
                "year": "2023-24"
            },
            {
                "title": "EngageMe: Assessing Student Engagement in Online Learning Environment Using Neuropsychological Tests",
                "DOI": "https://doi.org/10.1007/978-3-031-36336-8_23",
                "abstract": "In the proposed research, we investigated whether the standardized neuropsychological tests commonly used to assess attention can be used to measure students’ engagement in online learning settings. Accordingly, we employed 73 students in three clinically relevant neuropsychological tests to assess three types of attention. Students’ engagement performance, as evidenced by their facial video, was also annotated by three independent annotators. The manual annotations observed a high level of inter-annotator reliability (Krippendorffs’ Alpha of 0.864). Further, by obtaining a correlation value of 0.673 (Spearmans’ Rank Correlation) between manual annotation and neuropsychological tests score, our results show construct validity to prove neuropsychological test scores’ significance as a latent variable for measuring students’ engagement. Finally, using non-intrusive behavioral cues, including facial action unit and eye gaze data collected via webcam, we propose a machine learning method for engagement analysis in online learning settings, achieving a low mean squared error value (0.022). The findings suggest a neuropsychological test-based machine learning technique could effectively assess students’ engagement in online education.Access provided by Indraprastha Institute of Inf",
                "year": "2023-24"
            },
            {
                "title": "AttentioNet: Monitoring Student Attention Type in Learning with EEG-Based Measurement System",
                "DOI": "https://doi.org/10.1109/ACII59096.2023.10388212",
                "abstract": "Student attention is an indispensable input for uncovering their goals, intentions, and interests, which prove to be invaluable for a multitude of research areas, ranging from psychology to interactive systems. However, most existing methods to classify attention fail to model its complex nature. To bridge this gap, we propose AttentioNet, a novel Convolutional Neural Network-based approach that utilizes Electroencephalography (EEG) data to classify attention into five states: Selective, Sustained, Divided, Alternating, and relaxed state. We collected a dataset of 20 subjects through standard neuropsychological tasks to elicit different attentional states. The average across-student accuracy of our proposed model at this configuration is 92.3% (SD=3.04), which is well-suited for end-user applications. Our transfer learning-based approach for personalizing the model to individual subjects effectively addresses the issue of individual variability in EEG signals, resulting in improved performance and adaptability of the model for real-world applications. This represents a significant advancement in the field of EEG-based classification. Experimental results demonstrate that AttentioNet outperforms a popular EEGnet baseline (p-value < 0.05) in both subject-independent and subject-dependent settings, confirming the effectiveness of our proposed approach despite the limitations of our dataset. These results highlight the promising potential of AttentioNet for attention classification using EEG data.",
                "year": "2023-24"
            },
            {
                "title": "Multi-source transfer learning for facial emotion recognition using multivariate correlation analysis",
                "DOI": "https://doi.org/10.1038/s41598-023-48250-x",
                "abstract": "Deep learning techniques have proven to be effective in solving the facial emotion recognition (FER) problem. However, it demands a significant amount of supervision data which is often unavailable due to privacy and ethical concerns. In this paper, we present a novel approach for addressing the FER problem using multi-source transfer learning. The proposed method leverages the knowledge from multiple data sources of similar domains to inform the model on a related task. The approach involves the optimization of aggregate multivariate correlation among the source tasks trained on the source dataset, thus controlling the transfer of information to the target task. The hypothesis is validated on benchmark datasets for facial emotion recognition and image classification tasks, and the results demonstrate the effectiveness of the proposed method in capturing the group correlation among features, as well as being robust to negative transfer and performing well in few-shot multi-source adaptation. With respect to the state-of-the-art methods MCW and DECISION, our approach shows an improvement of 7% and 15% respectively.",
                "year": "2023-24"
            },
            {
                "title": "InMDb: Indian Movie Database for Emotion Analysis",
                "DOI": "https://doi.org/10.1145/3627631.3627665",
                "abstract": "Cinematic experiences, characterized by intricate audio-visual stimuli, foster profound emotional engagement. However, the correlation between audience emotions, physiological responses, film genres, and ratings, particularly in the underexplored Bollywood context, remains largely uncharted. Understanding this intricate interplay can provide filmmakers valuable insights for content adaptation. Addressing this research gap, we introduce \"InMDB: Indian Movie DataBase,\" a comprehensive multimodal dataset that examines emotional responses elicited by Bollywood trailers, using both self-reported measures and physiological data. Our meticulous statistical analysis of the dataset deepens the understanding of how emotions and their subsequent physiological responses correlate with, and potentially influence, film ratings and categories, offering novel insights into emotional engagement in the cinematic context.",
                "year": "2023-24"
            },
            {
                "title": "An EEG-Based Computational Model for Decoding Emotional Intelligence,Personality, and Emotions",
                "DOI": "https://doi.org/10.1109/TIM.2023.3347790",
                "abstract": "Emotional intelligence (EI), a critical aspect of regulating emotions and behavior in daily life, holds paramount significance in both psychology research and real-world applications. Understanding and assessing EI are essential for informed decision-making, nurturing relationships, and facilitating efficient communication. As human–computer interaction (HCI) continues to evolve, there is a growing need to develop systems capable of comprehending human emotions, personality traits, and moods through recognition models. This research endeavors to explore the potential of recognizing EI in the context of effective HCI. To address this challenge, we have developed a novel computational model based on electroencephalogram (EEG) data. Our work encompasses a carefully curated EEG dataset, featuring recordings from 40 participants who were exposed to a set of 16 emotional video clips selected from distinct quadrants of the valence-arousal (VA) space. Participants’ emotional responses were meticulously annotated through self-assessment of emotional dimensions for each video stimulus. In addition, participants’ feedback on the big-five personality traits and their responses to the trait emotional intelligence questionnaire (TEIQue) served as our ground truth for further analysis. Our study includes a comprehensive correlation analysis, using Pearson correlations to establish the relationships between personality traits and EI. Furthermore, we conducted EEG-based analysis to uncover connections between EEG signals and emotional attributes. Remarkably, our analysis reveals that EEG signals excel at capturing differences in EI levels. Leveraging machine learning algorithms, we have constructed binary classification models that yield average $F1$ scores of 0.72, 0.71, and 0.62 for emotions, personality traits, and EI, respectively. These experimental outcomes underscore the potential of EEG signals in the recognition of EI, personality traits, and emotions. We envision our proposed model as a foundational element in the development of effective HCI systems, enabling a deeper and better understanding of human behavior.",
                "year": "2023-24"
            },
            {
                "title": "Effecti-Net: A Multimodal Framework and Database for Educational Content Effectiveness Analysis",
                "DOI": "https://doi.org/10.1145/3636555.3636928",
                "abstract": "Amid the evolving landscape of education, evaluating the impact of educational video content on students remains a challenge. Existing methods for assessment often rely on heuristics and self-reporting, leaving room for subjectivity and limited insight. This study addresses this issue by leveraging physiological sensor data to predict student-perceived content effectiveness. Within the realm of educational content evaluation, prior studies focused on conventional approaches, leaving a gap in understanding the nuanced responses of students to educational materials. To bridge this gap, our research introduces a novel perspective, building upon previous work in multimodal physiological data analysis. Our primary contributions encompass two key elements. First, we present the ’Effecti-Net’ architecture, a sophisticated deep learning model that integrates data from multiple sensor modalities, including Electroencephalogram (EEG), Eye Tracker, Galvanic Skin Response (GSR), and Photoplethysmography (PPG). Second, we introduce the ’DECEP’ dataset, a repository comprising 597 minutes of multimodal sensor data. To assess the effectiveness of our approach, we benchmark it against conventional methods. Remarkably, our model achieves a lowest MSE of 0.1651 and MAE of 0.3544 on the DECEP dataset. It offers educators and content creators a comprehensive framework that promotes the development of more engaging educational content.",
                "year": "2023-24"
            },
            {
                "title": "Dhruv Verma, Sejal Bhalla, Dhruv Sahnan, Jainendra Shukla, and Aman Parnami. 2021. ExpressEar: Sensing Fine-Grained Facial Expressions with Earables. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 3, Article 129 (Sept 2021), 28 pages.",
                "DOI": "https://doi.org/10.1145/3478085",
                "abstract": "Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar's applicability in the real world and open up research opportunities to advance its practical adoption."
            },
            {
                "title": "J. Shukla et al., \"Contextual Emotion Learning Challenge,\" 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021), 2021, pp. 1-7.",
                "DOI": "10.1109/FG52635.2021.9667034",
                "abstract": "Emotion recognition via vision has been deeply associated with facial expressions, and the inference of emotions has, more often than not, been based on the same. However, context, both environmental and social, plays an imperative role in emotion recognition but has not been incorporated widely so far. The meaning of emotion might entirely switch when shifted from one setting to another if only facial expressions are taken into account. Moreover, there exists no study in the Indian context about the same. To cater to this issue, we generate and introduce the Indian Contextual Emotion Recognition (ICER) dataset based on the multi-ethnic Indian context. This paper summarises the Contextual Emotion Learning Challenge (CELC 2021) organized in conjunction with the 16th IEEE Conference on Automatic Face and Gesture Recognition (FG) 2021. We outline the tasks posed in the challenge, the novel dataset, along with its challenges and the evaluation method. Lastly, we conclude by discussing the possible future directions."
            },
            {
                "title": "K. Rana, R. Madaan and J. Shukla, \"Effect of Polite Triggers in Chatbot Conversations on User Experience across Gender, Age, and Personality,\" 2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN), 2021, pp. 813-819",
                "DOI": "https://ieeexplore.ieee.org/document/9515528",
                "abstract": "Chatbots are one of the emerging intelligent systems which interact with customers to solve different queries in a wide range of domain areas. During social interaction, politeness plays a vital role in achieving effective communication. Consequently, it becomes essential to understand how a chatbot’s politeness affects user experience during the interaction. To understand it, we conducted a between-subject user study with two chatbots where one of the chatbots employs polite triggers, and the other one replies intending to answer the queries. To introduce politeness in normal chatbot responses, we used the state-of-the-art tag and generate approach. We first analyzed how different personality traits influence the response of individual persons to polite triggers. In addition, we also investigated the effects of polite triggers among different genders and age groups using a cross-sectional analysis."
            },
            {
                "title": "B. Ashwini, V. Narayan, A. Bhatia and J. Shukla, \"Responsiveness towards robot-assisted interactions among pre-primary children of Indian ethnicity,\" 2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN), 2021, pp. 619-625. ",
                "DOI": "https://ieeexplore.ieee.org/document/9515520",
                "abstract": "Today’s world is undeniably technology-driven and children are the ones who adopt technology with ease. This fact could be leveraged to design assistive technologies for children using social robots as they are observed to be efficient pedagogical agents. Robotic technology is evolving rapidly and robots are designed to play social roles in education, health care and home assistance. However, there has been limited research focusing on the use of robotic technologies for designing interactions with children in the global south, owing to which the response behaviour towards robot-assisted interventions are unknown. To address this gap, we conducted a study to understand the response behaviour of Indian children of the age 3-6 years towards robot-assisted interventions during directive tasks. Our analysis shows that the children could follow the robot's instructions during the tasks and complete the tasks successfully. The exploratory outcomes also highlight the acceptance and benefits of using robotic assistants as a facilitator in education, cognitive therapies and healthcare."
            },
            {
                "title": "Singhal A., Goyal M., Shukla J., Mutharaju R. (2021) Feature Fused Human Activity Recognition Network (FFHAR-Net). In: Stephanidis C., Antona M., Ntoa S. (eds) HCI International 2021 - Posters. HCII 2021. Communications in Computer and Information Science, vol 1420. Springer, Cham. ",
                "DOI": "https://doi.org/10.1007/978-3-030-78642-7_72",
                "abstract": "With the advances in smart home technology and Internet of Things (IoT), there has been keen research interest in human activity recognition to allow service systems to understand human intentions. Recognizing human objectives by these systems without user intervention, results in better service, which is crucial to improve the user experience. Existing research approaches have focused primarily on probabilistic methods like Bayesian networks (for instance, the CRAFFT algorithm). Though quite versatile, these probabilistic models may be unable to successfully capture the possibly complex relationships between the input variables. To the best of our knowledge, a statistical study of features in a human activity recognition task, their relationships, etc., has not yet been attempted. To this end, we study the domain of human activity recognition to improve the state-of-the-art and present a novel neural network architecture for the task. It employs early fusion on different types of minimalistic features such as time and location to make extremely accurate predictions with a maximum micro F1-score of 0.98 on the Aruba CASAS dataset. We also accompany the model with a comprehensive study of the features. Using feature selection techniques like Leave-One-Out, we rank the features according to the information they add to deep learning models and make further inferences using the ranking obtained. Our empirical results show that the feature Previous Activity Performed is the most useful of all, surprisingly even more than time (the basis of activity scheduling in most societies). We use three Activities of Daily Living (ADL) datasets in different settings to empirically demonstrate the utility of our architecture. We share our findings along with the models and the source code."
            },
            {
                "title": "Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021. Exploring Semi-Supervised Learning for Predicting Listener Backchannels. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, Article 395, 1–12. ",
                "DOI": "https://doi.org/10.1145/3411764.3445449",
                "abstract": "Developing human-like conversational agents is a prime area in HCI research and subsumes many tasks. Predicting listener backchannels is one such actively-researched task. While many studies have used different approaches for backchannel prediction, they all have depended on manual annotations for a large dataset. This is a bottleneck impacting the scalability of development. To this end, we propose using semi-supervised techniques to automate the process of identifying backchannels, thereby easing the annotation process. To analyze our identification module’s feasibility, we compared the backchannel prediction models trained on (a) manually-annotated and (b) semi-supervised labels. Quantitative analysis revealed that the proposed semi-supervised approach could attain 95% of the former’s performance. Our user-study findings revealed that almost 60% of the participants found the backchannel responses predicted by the proposed model more natural. Finally, we also analyzed the impact of personality on the type of backchannel signals and validated our findings in the user-study."
            }
            
        ]
    },
    {
        "profInfo": {
            "image": "kalpana.jpg",
            "name": "Dr. Kalpana Shankhwar",
            "archive" : false
        },
        "projects": [
            {
                "title": "Finite element analysis results visualization of manual metal arc welding using an interactive mixed reality-based user interface",
                "DOI": "https://doi.org/10.1016/j.jmapro.2023.03.001",
                "abstract": "Welding is extensively used in manufacturing industries for various applications. However, residual stress is induced due to the non-uniform temperature distribution on the weld plates during the welding process, which significantly affects the fatigue strength. In addition, the non-uniform expansion and contraction of the weld and surrounding base metal cause structural distortion. The distortion affects final product quality and results in lower productivity. Therefore, the structural analysis of the welded component is significantly important. In this work, a mixed reality (MR)-based user interface was developed to overlay the finite element analysis (FEA) results on the real weld plates in real time for manual metal arc welding (MMAW). Since the numerical simulation using FEA requires a large number of computational resources, a gradient boosted regression tree (GBRT) model was trained to predict the residual stress and deformation results. Furthermore, a lookup table and a trilinear interpolation method were used to render the results based on users' input data using Microsoft HoloLens 2 in real time. The developed interactive MR-based user interface can help welders quickly predict and control the residual stress and welding distortion before the real welding process and help novices learn the relationship between the welding parameters and the induced residual stress and deformation."
            },
            {
                "title": "An interactive extended reality-based tutorial system for fundamental manual metal arc welding training",
                "DOI": "https://doi.org/10.1007/s10055-022-00626-6",
                "abstract": "Extended reality (XR) technology has been proven an effective human–computer interaction tool to increase the perception of presence. The purpose of this study is to develop an interactive XR-based welding tutorial system to enhance the learning and hands-on skills of novice welders. This study is comprised of two parts: (1) fundamental manual metal arc welding (MMAW) science and technology tutoring in a virtual reality (VR)-based environment, and (2) hands-on welding training in a mixed reality (MR)-based environment. Using the developed tutorial system, complicated welding process and the effects of welding process parameters on weld bead geometry can be clearly observed and comprehended by using a 3D interactive user interface. Visual aids and quantitative guidance are displayed in real time to guide novice welders through the correct welding procedure and help them to maintain a proper welding position. A user study was conducted to evaluate the learnability, workload, and usability of the system. Results show that users obtained significantly better performance by using the XR-based welding tutorial system, compared to those who were trained using the conventional classroom training method."
            },
            {
                "title": "A visuo-haptic extended reality–based training system for hands-on manual metal arc welding training",
                "DOI": " https://doi.org/10.1007/s00170-022-09328-4",
                "abstract": "Welding training has been an important job training process in the industry and usually demands a large amount of resources. In real practice, the strong magnetic force and intense heat during the welding processes often frighten novice welders. In order to provide safe and effective welding training, this study developed a visuo-haptic extended reality (VHXR)–based hands-on welding training system for training novice welders to perform a real welding task. Novice welders could use the VHXR-based system to perform a hands-on manual arc welding task, without exposure to high temperature and intense ultraviolet radiation. Real-time and realistic force and visual feedback are provided to help trainees to maintain a constant arc length, travel speed, and electrode angle. Compared to the traditional video training, users trained using the VHXR-based welding training system significantly demonstrated better performance in real welding tasks. Trainees were able to produce better-quality joints by performing smoother welding with less mistakes, inquiry times, and hints."

            },
            {
                "title": "An augmented reality-based training system with a natural user interface for manual milling operations",
                "DOI": "https://doi.org/10.1007/s10055-019-00415-8",
                "abstract": "This study developed an augmented reality (AR)-based training system for conventional manual milling operations. An Intel RealSense R200 depth camera and a Leap Motion controller were mounted on an HTC Vive head-mounted display to allow users freely walk around in a room-size AR environment to operate a full-size virtual milling machine with their barehands, using their natural operation behaviors, as if they were operating a real milling machine in the real world, without additional worn or handheld devices. GPU parallel computing was used to handle dynamic occlusions and accelerate the machining simulation to achieve a real-time simulation. Using the developed AR-based training system, novices can receive a hands-on training in a safe environment, without any injury or damage. User test results showed that using the developed AR-based training resulted in lower failure rates and inquiry times than using video training. Users also commented that the AR-based training was interesting and helpful for novices to learn the basic manual milling operation techniques."
                
            },
            {
                "title": "A review of heat source and resulting temperature distribution in arc welding",
                "DOI": "https://doi.org/10.1007/s10973-022-11589-w",
                "abstract": "Thermal analysis is one of the cardinal studies essential for arc welding processes. Thermal field and temperature distribution in arc welds affect the quality of welds as they govern the microstructural and thermo-mechanical properties. Therefore, thorough understanding of the thermal behaviour in arc welds is an absolute necessity. Significant efforts have been made in the past to determine the temperature field associated with arc welding. However, for accurate determination of the temperature field/distribution, it is necessary to understand the heat source which influences the temperature distribution in welds. Rosenthal reported the first concept of modelling the heat source, which was then improvised and new models have been instituted through the years. This review article summarizes a collective study made on the heat source and the resulting temperature distribution in arc welds. Numerous methods have been developed to conduct transient temperature distribution studies on arc welds. Analytical approaches with constant material properties, numerical approaches with variable material properties, infrared imaging systems, machine vision systems with soft computing, etc. have been developed to facilitate understanding of transient temperature in arc welds. We first summarize heat source studies followed by literatures on various techniques and methods devoted to transient temperature investigations. Eventually, latest methods used for thermal studies, such as image processing, machine learning and intelligent systems are summarized and discussed."
            },
            {
                "title": "Empirical investigation of environmental characteristic of 3-D additive manufacturing process based on slice thickness and part orientation",
                "DOI": "https://doi.org/10.1016/j.measurement.2016.03.006",
                "abstract": "The significant amount of research has been done in improving the mechanical properties (compressive strength), dimensional accuracy (length, height and width), and build time of the components manufactured from the additive manufacturing process. In contrast to this, the research in the optimization of environmental characteristic i.e. energy consumption for the additive manufacturing processes such as selective laser sintering (SLS), and selective laser melting (SLM) needs significant attention. These processes intakes the significant portion of input laser energy for driving the laser system, heating system and other machine components. With world moving towards globalization of additive manufacturing processes, the optimization of laser energy consumption thus become a necessity from productivity and as well as an environmental perspective. Therefore, the present work performs the empirical investigation by proposing the optimization framework in modelling of laser energy consumption of the SLS process. The experimental procedure involves the computation of energy consumption by measuring the total area of sintering. The optimization framework when applied on the experimental data generates the functional expression for laser energy consumption which suggests that the slice thickness is a vital parameter in optimizing it. The implications arising from the study is discussed."
            },
            {
                "title": "Investigation of the joint length of weldment of environmental-friendly magnetic pulse welding process",
                "DOI": "https://doi.org/10.1007/s00170-016-8634-0",
                "abstract": "Among various sustainable processes, the magnetic pulse welding (MPW) is gaining popularity over the years due to its ability of joining two similar/dissimilar metals using the electromagnetic forces. Past studies extensively reported the use of experiments but limited numerical modeling based on finite element analysis for studying the quality of the weld with respect to individual and/or combined effect of impact velocity, impact angle, current, frequency, and electromagnetic force distribution. One such important application of this process is in the refrigerant tube where determining the desired Al/Cu joint length is essential. It was noticed that the extensive studies were done for selecting the desired joint length along the circumferential direction under specific materials, while very limited studies were carried on the joint length along the longitudinal direction. Therefore, the present work introduces the experimental and numerical framework based on genetic programming (GP) to measure and quantify the joint length along the longitudinal direction for refrigerant tube as a function of the gap between coil and outer tube, positions of the tube and impact angle. Further, the performance of the three complexity terms used in objective functions of GP was investigated in the formation of joint length models. The main and interactive relationships between the joint length and gap between coil and outer tube, positions of the tube, and impact angle are revealed. The experiment findings, model formulation, and the relationships of joint length will assist experts in monitoring the MPW effectively to obtain a high quality weld characteristic."
            }
        ]
    },
   
    {
        "profInfo": {
            "image": "ratn.png",
            "name": "Dr. Rajiv R Shah",
            "archive" : false
        },
        "projects": [
            {
                "title": "Automatic Essay Scoring Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses",
                "abstract": "Deep-learning based Automatic Essay Scoring (AES) systems are being actively used in various high-stake applications in education and testing. However, little research has been put to understand and interpret the black-box nature of deep-learning-based scoring algorithms. While previous studies indicate that scoring models can be easily fooled, in this paper, we explore the reason behind their surprising adversarial brittleness. We utilize recent advances in interpretability to find the extent to which features such as coherence, content, vocabulary, and relevance are important for automated scoring mechanisms. We use this to investigate the oversensitivity (i.e., large change in output score with a little change in input essay content) and overstability (i.e., little change in output scores with large changes in input essay content) of AES. Our results indicate that autoscoring models, despite getting trained as “end-to-end” models with rich contextual embeddings such as BERT, behave like bag-of-words models. A few words determine the essay score without the requirement of any context making the model largely overstable. This is in stark contrast to recent probing studies on pre-trained representation learning models, which show that rich linguistic features such as parts-of-speech and morphology are encoded by them. Further, we also find that the models have learnt dataset biases, making them oversensitive. The presence of a few words with high co-occurrence with a certain score class makes the model associate the essay sample with that score. This causes score changes in ∼95% of samples with an addition of only a few words. To deal with these issues, we propose detection-based protection models that can detect oversensitivity and samples causing overstability with high accuracies. We find that our proposed models are able to detect unusual attribution patterns and flag adversarial samples successfully.",
                "DOI": "https://journals.uic.edu/ojs/index.php/dad/article/view/12448",
                "year": "2023-24"
            },
            {
                "title": "Meta Perturbed Re-Id Defense",
                "abstract": "Adversarial attacks have gained significant attention in object re-identification (Re-Id). However, very few works target defense, and they primarily adopt adversarial training. While adversarial training has been shown to be a major line of defense, we observe that vanilla adversarial training alone does not provide a robust defense against adversarial attacks. Towards this, we propose a novel meta perturbed defense algorithm for Re-Id task. Our contributions are, (i) we introduce anisotropic and isotropic perturbations to design a stochastic neural network, and train it with a novel meta-learning strategy with tasks as vanilla, perturbed, and perturbed-adversarial training; (ii) we show the generalizability of our model against various unseen attacks; and (iii) we derive a novel feature covariance alignment (FCA) loss which gives us a high clean performance while providing robustness against different attacks. Extensive experiments on Market-1501, MSMT17 and VeRi-776 reveal SOTA performance. 1",
                "DOI": "https://ieeexplore.ieee.org/document/10220009",
                "year": "2023-24"
            },
            {
                "title": "Hindi Chatbot for Supporting Maternal and Child Health Related Queries in Rural India",
                "abstract": "In developing countries like India, doctors and healthcare professionals working in public health spend significant time answering health queries that are fact-based and repetitive. Therefore, we propose an automated way to answer maternal and child health-related queries. A database of Frequently Asked Questions (FAQs) and their corresponding answers generated by experts is curated from rural health workers and young mothers. We develop a Hindi chatbot that identifies k relevant Question and Answer (QnA) pairs from the database in response to a healthcare query (q) written in Devnagri script or Hindi-English (Hinglish) code-mixed script. The curated database covers 80% of all the queries that a user of our study is likely to ask. We experimented with (i) rule-based methods, (ii) sentence embeddings, and (iii) a paraphrasing classifier, to calculate the q-Q similarity. We observed that paraphrasing classifier gives the best result when trained first on an open-domain text and then on the healthcare domain. Our chatbot uses an ensemble of all three approaches. We observed that if a given q can be answered using the database, then our chatbot can provide at least one relevant QnA pair among its top three suggestions for up to 70% of the queries.",
                "DOI": "https://aclanthology.org/2023.clinicalnlp-1.9",
                "year": "2023-24"
            },
            {
                "title": "Persuasion Strategies in Advertisements",
                "abstract": "Modeling what makes an advertisement persuasive, i.e., eliciting the desired response from consumer, is critical to the\nstudy of propaganda, social psychology, and marketing. Despite its importance, computational modeling of persuasion\nin computer vision is still in its infancy, primarily due to\nthe lack of benchmark datasets that can provide persuasion-strategy labels associated with ads. Motivated by persuasion literature in social psychology and marketing, we introduce an extensive vocabulary of persuasion strategies and\nbuild the first ad image corpus annotated with persuasion\nstrategies. We then formulate the task of persuasion strategy prediction with multi-modal learning, where we design\na multi-task attention fusion model that can leverage other\nad-understanding tasks to predict persuasion strategies. The\ndataset also provides image segmentation masks, which labels persuasion strategies in the corresponding ad images on\nthe test split. We publicly release our code and dataset at https://midas-research.github.io/persuasion-advertisements/.",
                "DOI": "https://ojs.aaai.org/index.php/AAAI/article/view/25076",
                "year": "2023-24"
            },
            {
                "title": "Mask-Net: Learning Context Aware Invariant Features Using Adversarial Forgetting (Student Abstract)",
                "abstract": "Training a robust system, e.g., Speech to Text (STT), requires large datasets. Variability present in the dataset, such as unwanted nuances and biases, is the reason for the need for large datasets to learn general representations. In this work, we propose a novel approach to induce invariance using adversarial forgetting (AF). Our initial experiments on learning invariant features such as accent on the STT task achieve better generalizations in terms of word error rate (WER) compared to traditional models. We observe an absolute improvement of 2.2% and 1.3%  on out-of-distribution and in-distribution test sets, respectively.",
                "DOI": "https://ojs.aaai.org/index.php/AAAI/article/view/27047",
                "year": "2023-24"
            },
            {
                "title": "Analysing the Masked Predictive Coding Training Criterion for Pre-Training a Speech Representation Model",
                "abstract": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion’s that directly preserves both speaker and content information at various layers of a learnt model.",
                "DOI": "https://ieeexplore.ieee.org/document/10096167",
                "year": "2023-24"
            },
            {
                "title": "Effect of Feedback on Drug Consumption Disclosures on Social Media",
                "abstract": "Deaths due to drug overdose in the US have doubled in the last decade. Drug-related content on social media has also exploded in the same time frame. The pseudo-anonymous nature of social media platforms enables users to discourse about taboo and sometimes illegal topics like drug consumption. User-generated content (UGC) about drugs on social media can be used as an online proxy to detect offline drug consumption. UGC also gets exposed to the praise and criticism of the community. Law of effect proposes that positive reinforcement on an experience can incentivize the users to engage in the experience repeatedly. Therefore, we hypothesize that positive community feedback on a user's online drug consumption disclosure will increase the probability of the user doing an online drug consumption disclosure post again. To this end, we collect data from 10 drug-related subreddits. First, we build a deep learning model to classify UGC as indicative of drug consumption offline or not, and analyze the extent of such activities. Further, we use matching-based causal inference techniques to unravel community feedback's effect on users' future drug consumption behavior. We discover that 84% of posts and 55% comments on drug-related subreddits indicate real-life drug consumption. Users who get positive feedback generate up to two times more drugs consumption content in the future. Finally, we conducted an anonymous user study on drug-related subreddits to compare members' opinions with our experimental findings and show that user tends to underestimate the effect community peers can have on their decision to interact with drugs.",
                "DOI": "https://ojs.aaai.org/index.php/ICWSM/article/view/22158",
                "year": "2023-24"
            },
            {
                "title": "CiteCaseLAW: Citation Worthiness Detection in Caselaw for Legal Assistive Writing",
                "abstract": "In legal document writing, one of the key elements is properly citing the case laws and other sources to substantiate claims and arguments. Understanding the legal domain and identifying appropriate citation context or cite-worthy sentences are challenging tasks that demand expensive manual annotation. The presence of jargon, language semantics, and high domain specificity makes legal language complex, making any associated legal task hard for automation. The current work focuses on the problem of citation-worthiness identification. It is designed as the initial step in today's citation recommendation systems to lighten the burden of extracting an adequate set of citation contexts. To accomplish this, we introduce a labeled dataset of 178M sentences for citation-worthiness detection in the legal domain from the Caselaw Access Project (CAP). The performance of various deep learning models was examined on this novel dataset. The domain-specific pre-trained model tends to outperform other models, with an 88% F1-score for the citation-worthiness detection task.",
                "DOI": "http://arxiv.org/abs/2305.03508",
                "year": "2023-24"
            },
            {
                "title": "Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser",
                "abstract": "Certified defense methods against adversarial perturbations have been recently investigated in the black-box setting with a zeroth-order (ZO) perspective. However, these methods suffer from high model variance with low performance on high-dimensional datasets due to the ineffective design of the denoiser and are limited in their utilization of ZO techniques. To this end, we propose a certified ZO preprocessing technique for removing adversarial perturbations from the attacked image in the black-box setting using only model queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets. We propose a novel black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our RDUNet to the black-box model, ensuring black-box defense. We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model. We perform extensive experiments on four classification datasets, CIFAR-10, CIFAR-10, Tiny Imagenet, STL-10, and the MNIST dataset for image reconstruction tasks. Our proposed defense methods ZO-RUDS and ZO-AE-RUDS beat SOTA with a huge margin of $35\\%$ and $9\\%$, for low dimensional (CIFAR-10) and with a margin of $20.61\\%$ and $23.51\\%$ for high-dimensional (STL-10) datasets, respectively.",
                "DOI": "http://arxiv.org/abs/2304.06430",
                "year": "2023-24"
            },
            {
                "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
                "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
                "DOI": "http://arxiv.org/abs/2309.00359",
                "year": "2023-24"
            },
            {
                "title": "Multilingual Coreference Resolution in Low-resource South Asian Languages",
                "abstract": "Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on our test-split of Hindi golden set. This study is the first to evaluate an end-to-end coreference resolution model on a Hindi golden set. Furthermore, this work underscores the limitations of current coreference evaluation metrics when applied to datasets with split antecedents, advocating for the development of more suitable evaluation metrics.",
                "DOI": "http://arxiv.org/abs/2402.13571",
                "year": "2023-24"
            },
            {
                "title": "Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-Supervised Setting",
                "abstract": "This paper introduces a novel objective function for quality mean opinion score (MOS) prediction of unseen speech synthesis systems. The proposed function measures the similarity of relative positions of predicted MOS values, in a mini-batch, rather than the actual MOS values. That is the partial rank similarity is measured (\\mathcalPRS) rather than the individual MOS values as with the L1 loss. Our experiments on out-of-domain speech synthesis systems demonstrate that the \\mathcalP R S outperforms L1 loss in zero-shot and semi-supervised settings, exhibiting stronger correlation with ground truth. These findings highlight the importance of considering rank order, as done by \\mathcalPRS, when training MOS prediction models. We also argue that mean squared error and linear correlation coefficient metrics may be unreliable for evaluating MOS prediction models. In conclusion, \\mathcalP R S-trained models provide a robust framework for evaluating speech quality and offer insights for developing high-quality speech synthesis systems. Code and models are available at github.com/nii-yamagishilab/partial_rank_similarity/",
                "DOI": "https://ieeexplore.ieee.org/document/10389797",
                "year": "2023-24"
            },
            {
                "title": "RanLayNet: A Dataset for Document Layout Detection used for Domain Adaptation and Generalization",
                "abstract": "Large ground-truth datasets and recent advances in deep learning techniques have been useful for layout detection. However, because of the restricted layout diversity of these datasets, training on them requires a sizable number of annotated instances, which is both expensive and time-consuming. As a result, differences between the source and target domains may significantly impact how well these models function. To solve this problem, domain adaptation approaches have been developed that use a small quantity of labeled data to adjust the model to the target domain. In this research, we introduced a synthetic document dataset called RanLayNet, enriched with automatically assigned labels denoting spatial positions, ranges, and types of layout elements. The primary aim of this endeavor is to develop a versatile dataset capable of training models with robustness and adaptability to diverse document formats. Through empirical experimentation, we demonstrate that a deep layout identification model trained on our dataset exhibits enhanced performance compared to a model trained solely on actual documents. Moreover, we conduct a comparative analysis by fine-tuning inference models using both PubLayNet and IIIT-AR-13K datasets on the Doclaynet dataset. Our findings emphasize that models enriched with our dataset are optimal for tasks such as achieving 0.398 and 0.588 mAP95 score in the scientific document domain for the TABLE class.",
                "DOI": "https://dl.acm.org/doi/10.1145/3595916.3626448",
                "year": "2023-24"
            },
            {
                "title": "SciPhyRAG - Retrieval Augmentation to Improve LLMs on Physics Q &A",
                "abstract": "Large Language Models (LLMs) have showcased their value across diverse domains, yet their efficacy in computationally intensive tasks remains limited in accuracy. This paper introduces a comprehensive methodology to construct a resilient dataset focused on High School Physics, leveraging retrieval augmentation. Subsequent finetuning of a Large Language Model through instructional calibration is proposed to elevate outcome precision and depth. The central aspiration is reinforcing LLM efficiency in educational contexts, facilitating more precise, well-contextualized, and informative results. By bridging the gap between LLM capabilities and the demands of complex educational tasks, this approach seeks to empower educators and students alike, offering enhanced support and enriched learning experiences. Compared to Vicuna-7b, the finetuned retrieval augmented model SciPhy-RAG exhibits a 16.67% increase in BERTScore and 35.2% increase on ROUGE-2 scores. This approach has the potential to be used to reshape Physics Q &A by LLMs and has a lasting impact on their use for Physics education. Furthermore, the data sets released can be a reference point for future research and educational domain tasks such as Automatic Evaluation and Question Generation.",
                "DOI": "10.1007/978-3-031-49601-1_4",
                "year": "2023-24"
            },
            {
                "title": "Revolutionizing High School Physics Education: A Novel Dataset",
                "abstract": "Despite the growing capabilities of Large Language Models (LLMs) in various domains, their proficiency in addressing domain-specific high-school physics questions remains an unexplored area. In this study, we present a pioneering data set curated from NCERT exemplar solutions strategically designed to facilitate the use of LLMs to solve school physics questions. Originally comprising 766 questions accompanied by LaTeX representations, the dataset underwent a sophisticated augmentation process that expanded its scope to an impressive 7,983 questions. The augmentation employed innovative techniques which effectively broaden the dataset’s coverage. The dataset, prioritizing text-based questions, is formatted as JSON objects detailing instructions, inputs, and outputs. Post evaluation, we noted significant scores: METEOR at 0.282 and BERTScore F1 at 0.833, indicating a close alignment between generated and reference texts.",
                "DOI": "10.1007/978-3-031-49601-1_5",
                "year": "2023-24"
            },
            {
                "title": "Context-Enhanced Language Models for Generating Multi-paper Citations",
                "abstract": "Citation text plays a pivotal role in elucidating the connection between scientific documents, demanding an in-depth comprehension of the cited paper. Constructing citations is often time-consuming, requiring researchers to delve into extensive literature and grapple with articulating relevant content. To address this challenge, the field of citation text generation (CTG) has emerged. However, while earlier methods have primarily centered on creating single-sentence citations, practical scenarios frequently necessitate citing multiple papers within a single paragraph. To bridge this gap, we propose a method that leverages Large Language Models (LLMs) to generate multi-citation sentences. Our approach involves a single source paper and a collection of target papers, culminating in a coherent paragraph containing multi-sentence citation text. Furthermore, we introduce a curated dataset named MCG-S2ORC, composed of English-language academic research papers in Computer Science, showcasing multiple citation instances. In our experiments, we evaluate three LLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this endeavor. Additionally, we exhibit enhanced performance by integrating knowledge graphs from target papers into the prompts for generating citation text. This research underscores the potential of harnessing LLMs for citation generation, opening a compelling avenue for exploring the intricate connections between scientific documents.",
                "DOI": "10.1007/978-3-031-49601-1_6",
                "year": "2023-24"
            },
            {
                "title": "GEC-DCL: Grammatical Error Correction Model with Dynamic Context Learning for Paragraphs and Scholarly Papers",
                "abstract": "Over the past decade, there has been noteworthy progress in the field of Automatic Grammatical Error Correction (GEC). Despite this growth, current GEC models possess limitations as they primarily concentrate on single sentences, neglecting the significance of contextual understanding in error correction. While a few models have begun to factor in context alongside target sentences, they frequently depend on inflexible boundaries, which leads to the omission of vital information necessary for rectifying certain errors. To address this issue, we introduce the Dynamic Context Learner (DCL) model, which identifies optimal breakpoints within paragraphs or documents to retain maximum context. Our method surpasses those employing fixed sequence lengths or assuming a limited number of preceding sentences as context. Through extensive evaluation on the CoNLL-2014, BEA-Dev, and FCE-Test datasets, we substantiate the efficacy of our approach, achieving substantial F$$_{0.5}$$0.5score enhancements: 77% increase, 19.61% boost, and 10.49% rise respectively, compared to state-of-the-art models. Furthermore, we contrast our model’s performance with LLaMA’s GEC capabilities. We extend our investigation to scientific writing encompassing various context lengths and validate our technique on the GEC S2ORC dataset, yielding cutting-edge results in scholarly publications.",
                "DOI": "10.1007/978-3-031-49601-1_7",
                "year": "2023-24"
            },
            {
                "title": "KG-CTG: Citation Generation Through Knowledge Graph-Guided Large Language Models",
                "abstract": "Citation Text Generation (CTG) is a task in natural language processing (NLP) that aims to produce text that accurately cites or references a cited document within a source document. In CTG, the generated text draws upon contextual cues from both the source document and the cited paper, ensuring accurate and relevant citation information is provided. Previous work in the field of citation generation is mainly based on the text summarization of documents. Following this, this paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation. Also, we have shown the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers. To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language. Vicuna performs best for this task with 14.15 Meteor, 12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by including knowledge graphs.",
                "DOI": "10.1007/978-3-031-49601-1_3",
                "year": "2023-24"
            },
            {
                "title": "Explaining Finetuned Transformers on Hate Speech Predictions Using Layerwise Relevance Propagation",
                "abstract": "Explainability of model predictions has become imperative for architectures that involve fine-tuning of a pretrained transformer encoder for a downstream task such as hate speech detection. In this work, we compare the explainability capabilities of three post-hoc methods on the HateXplain benchmark with different encoders. Our research is the first work to evaluate the effectiveness of Layerwise Relevance Propagation (LRP) as a post-hoc method for fine-tuned transformer architectures used in hate speech detection. The analysis revealed that LRP tends to perform less effectively than the other two methods across various explainability metrics. A random rationale generator was found to be providing a better interpretation than the LRP method. Upon further investigation, it was discovered that the LRP method assigns higher relevance scores to the initial tokens of the input text because fine-tuned encoders tend to concentrate the text information in the embeddings corresponding to early tokens of the text. Therefore, our findings demonstrate that LRP relevance values at the input of fine-tuning layers are not a good representative of the rationales behind the predicted score.",
                "DOI": "10.1007/978-3-031-49601-1_14",
                "year": "2023-24"
            },
            {
                "title": "Correction: Expert-augmented automated machine learning optimizes hemodynamic predictors of spinal cord injury outcome",
                "abstract": "[This corrects the article DOI: 10.1371/journal.pone.0265254.] Artificial intelligence and machine learning (AI/ML) is becoming increasingly more accessible to biomedical researchers with significant potential to transform biomedicine through optimization of highly-accurate predictive models and enabling better understanding of disease biology. Automated machine learning (AutoML) in particular is positioned to democratize artificial intelligence (AI) by reducing the amount of human input and ML expertise needed. However, successful translation of AI/ML in biomedicine requires moving beyond optimizing only for prediction accuracy and towards establishing reproducible clinical and biological inferences. This is especially challenging for clinical studies on rare disorders where the smaller patient cohorts and corresponding sample size is an obstacle for reproducible modeling results. Here, we present a model-agnostic framework to reinforce AutoML using strategies and tools of explainable and reproducible AI, including novel metrics to assess model reproducibility. The framework enables clinicians to interpret AutoML-generated models for clinical and biological verifiability and consequently integrate domain expertise during model development. We applied the framework towards spinal cord injury prognostication to optimize the intraoperative hemodynamic range during injury-related surgery and additionally identified a strong detrimental relationship between intraoperative hypertension and patient outcome. Furthermore, our analysis captured how evolving clinical practices such as faster time-to-surgery and blood pressure management affect clinical model development. Altogether, we illustrate how expert-augmented AutoML improves inferential reproducibility for biomedical discovery and can ultimately build trust in AI processes towards effective clinical integration.",
                "DOI": "10.1371/journal.pone.0294081",
                "year": "2023-24"
            },
            {
                "title": "Behavior Optimized Image Generation",
                "abstract": "The last few years have witnessed great success on image generation, which has crossed the acceptance thresholds of aesthetics, making it directly applicable to personal and commercial applications. However, images, especially in marketing and advertising applications, are often created as a means to an end as opposed to just aesthetic concerns. The goal can be increasing sales, getting more clicks, likes, or image sales (in the case of stock businesses). Therefore, the generated images need to perform well on these key performance indicators (KPIs), in addition to being aesthetically good. In this paper, we make the first endeavor to answer the question of \"How can one infuse the knowledge of the end-goal within the image generation process itself to create not just better-looking images but also \"better-performing'' images?''. We propose BoigLLM, an LLM that understands both image content and user behavior. BoigLLM knows how an image should look to get a certain required KPI. We show that BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this task, demonstrating that while these state-of-the-art models can understand images, they lack information on how these images perform in the real world. To generate actual pixels of behavior-conditioned images, we train a diffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward. We show the performance of the overall pipeline on two datasets covering two different behaviors: a stock dataset with the number of forward actions as the KPI and a dataset containing tweets with the total likes as the KPI, denoted as BoigBench. To advance research in the direction of utility-driven image generation and understanding, we release BoigBench, a benchmark dataset containing 168 million enterprise tweets with their media, brand account names, time of post, and total likes.",
                "DOI": "https://arxiv.org/abs/2311.10995v1",
                "year": "2023-24"
            },
            {
                "title": "TC-OCR: TableCraft OCR for Efficient Detection & Recognition of Table Structure & Content",
                "abstract": "The automatic recognition of tabular data in document images presents a significant challenge due to the diverse range of table styles and complex structures. Tables offer valuable content representation, enhancing the predictive capabilities of various systems such as search engines and Knowledge Graphs. Addressing the two main problems, namely table detection (TD) and table structure recognition (TSR), has traditionally been approached independently. In this research, we propose an end-to-end pipeline that integrates deep learning models, including DETR, Cascade TabNet, and PP OCR v2, to achieve comprehensive image-based table recognition. This integrated approach effectively handles diverse table styles, complex structures, and image distortions, resulting in improved accuracy and efficiency compared to existing methods like Table Transformer. Our system achieves simultaneous table detection, table structure recognition, and table content recognition (TCR), preserving table structures and accurately extracting tabular data from document images. The integration of multiple models addresses the intricacies of table recognition, making our approach a promising solution for image-based table understanding, data extraction, and information retrieval applications. Our proposed approach achieves an IOU of 0.96 and an OCR Accuracy of 78%, showcasing a remarkable improvement of approximately 25% in the OCR Accuracy compared to the previous Table Transformer approach.",
                "DOI": "https://dl.acm.org/doi/10.1145/3606040.3617444",
                "year": "2023-24"
            },
            {
                "title": "IndIE: A Multilingual Open Information Extraction Tool For Indic Languages",
                "abstract": "Open Information Extraction (OIE) is the process of extracting informative facts from opendomain natural language text. A multilingualOIE tool, IndIE, has been proposed, which performs chunking, creates a Merged-phrase Dependency Tree (MDT), and generates triplesusing hand-crafted rules. It is observed thatfine-tuned transformer-based chunker outperforms other traditional methods of chunking.A benchmark called Hindi-BenchIE has alsobeen developed for automatically evaluatingHindi triples. The developed OIE tool, IndIE,has been automatically evaluated on the goldentriples of 112 Hindi sentences. Compared toother multilingual methods, the IndIE methodgenerates more meaningful triples with 0.51F1-score. It is observed that IndIE generatesmore fine-grained triples than other methods.It is conjectured that IndIE has the ability togenerate meaningful triples for Urdu, Tamil,and Telugu sentences as well because the developed chunker is shown to generalize acrossvarious natural languages, and the triple generation rules are based on dependency relationsthat are common to the aforementioned Indiclanguages.",
                "DOI": "https://aclanthology.org/2023.findings-ijcnlp.28",
                "year": "2023-24"
            },
            {
                "title": "Emotionally Enhanced Talking Face Generation",
                "abstract": "Several works have developed end-to-end pipelines for generating lip-synced talking faces with real-world applications, such as teaching and language translation in videos. However, these prior works fail to create realistic-looking videos since they focus little on people's expressions and emotions. Moreover, these methods' effectiveness largely depends on the faces in the training dataset, which means they may not perform well on unseen faces. To mitigate this, we build a talking face generation framework conditioned on a categorical emotion to generate videos with appropriate expressions, making them more realistic and convincing. With a broad range of six emotions, i.e., happiness, sadness, fear, anger, disgust, and neutral, we show that our model can adapt to arbitrary identities, emotions, and languages. Our proposed framework has a user-friendly web interface with a real-time experience for talking face generation with emotions. We also conduct a user study for subjective evaluation of our interface's usability, design, and functionality. Project page: \\hrefhttps://midas.iiitd.edu.in/emo/ https://midas.iiitd.edu.in/emo/ .",
                "DOI": "https://dl.acm.org/doi/10.1145/3607541.3616812",
                "year": "2023-24"
            },
            {
                "title": "EAAI-23 Blue Sky Ideas in Artificial Intelligence Education from the AAAI/ACM SIGAI New and Future AI Educator Program",
                "abstract": "The 13th Symposium on Educational Advances in Artificial Intelligence (EAAI-23), co-chaired by Michael Guerzhoy, Marion Neumann, and Pat Virtue, continued the tradition of the AAAI/ACM SIGAI New and Future AI Educator (NFAIED) Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia).This paper is a collection of the \"blue sky\" essays of the 2023 NFAIED awardees, intended to help motivate discussion around various current and important issues in AI education.",
                "DOI": "https://dl.acm.org/doi/10.1145/3609468.3609474",
                "year": "2023-24"
            },
            {
                "title": "Exploring Graph Neural Networks for Indian Legal Judgment Prediction",
                "abstract": "The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipating connections between two specified nodes. By harnessing the capabilities of graph neural networks and incorporating fairness analyses, this research aims to contribute insights towards streamlining the adjudication process, enhancing judicial efficiency, and fostering a more equitable legal landscape, ultimately alleviating the strain imposed by mounting case backlogs. Our best-performing model with XLNet pre-trained embeddings as its features gives the macro F1 score of 75% for the LJP task. For link prediction, the same set of features is the best performing giving ROC of more than 80%",
                "DOI": "http://arxiv.org/abs/2310.12800",
                "year": "2023-24"
            },
            {
                "title": "H-AES: Towards Automated Essay Scoring for Hindi",
                "abstract": "The use of Natural Language Processing (NLP) for Automated Essay Scoring (AES) has been well explored in the English language, with benchmark models exhibiting performance comparable to human scorers. However, AES in Hindi and other low-resource languages remains unexplored. In this study, we reproduce and compare state-of-the-art methods for AES in the Hindi domain. We employ classical feature-based Machine Learning (ML) and advanced end-to-end models, including LSTM Networks and Fine-Tuned Transformer Architecture, in our approach and derive results comparable to those in the English language domain. Hindi being a low-resource language, lacks a dedicated essay-scoring corpus. We train and evaluate our models using translated English essays and empirically measure their performance on our own small-scale, real-world Hindi corpus. We follow this up with an in-depth analysis discussing prompt-specific behavior of different language models implemented.",
                "DOI": "https://ojs.aaai.org/index.php/AAAI/article/view/26894",
                "year": "2023-24"
            },
            {
                "title": "On Comparing Fair Classifiers under Data Bias",
                "abstract": "In this paper, we consider a theoretical model for injecting data bias, namely, under-representation and label bias (Blum & Stangl, 2019). We empirically study the effect of varying data biases on the accuracy and fairness of fair classifiers. Through extensive experiments on both synthetic and real-world datasets (e.g., Adult, German Credit, Bank Marketing, COMPAS), we empirically audit pre-, in-, and post-processing fair classifiers from standard fairness toolkits for their fairness and accuracy by injecting varying amounts of under-representation and label bias in their training data (but not the test data). Our main observations are: 1. The fairness and accuracy of many standard fair classifiers degrade severely as the bias injected in their training data increases, 2. A simple logistic regression model trained on the right data can often outperform, in both accuracy and fairness, most fair classifiers trained on biased training data, and 3. A few, simple fairness techniques (e.g., reweighing, exponentiated gradients) seem to offer stable accuracy and fairness guarantees even when their training data is injected with under-representation and label bias. Our experiments also show how to integrate a measure of data bias risk in the existing fairness dashboards for real-world deployments.",
                "DOI": "http://arxiv.org/abs/2302.05906",
                "year": "2023-24"
            },
            {
                "title": "Generative and adversarial learning for object recognition",
                "abstract": "Generative modeling and adversarial learning have significantly advanced the field of computer vision, particularly in object recognition and synthesis, unsupervised domain adaptation, and adversarial attacks and defenses. These techniques have enabled the creation of more accurate and robust models for critical applications. In particular, we develop algorithms for fine-grained object recognition (Re-ID) and classification tasks. Re-ID involves matching objects across non-overlapping cameras, which is challenging due to visual recognition hurdles like pose change, occlusion, illumination variation, low resolution, and modality differences. On the other hand, object classification is another aim to categorize input data into pre-defined classes, using patterns learned from training data. In this context, our thesis is motivated by the potential of generative modelling to synthesize novel human views, which can be used for unsupervised learning of Re ID models. Unsupervised Re-ID suffers from domain discrepancies between labeled source and unlabeled target domains. Existing methods adapt the model using aug mented samples, either by translating source samples or assigning pseudo labels to the target. However, translation methods may lose identity details, while label assignment may give noisy labels. Our approach is distinct from other methods in that it decou ples the ID and non-ID features in a cyclic manner, which promotes better adaptation to pose and background, thereby resulting in richer novel views. This approach could improve the accuracy of Re-ID models for the unlabeled target domain, thus enhancing their robustness in real-world settings. Furthermore, we aim to analyze the robustness of Re-ID and classification models and propose adversarial attack and defense methods to enhance their reliability. Adver sarial attacks are a malicious technique that manipulates input data to cause machine learning models to make incorrect predictions or classifications. Adversarial defense methods, including adversarial training, certified defense, and detection mechanisms, are used to protect models from such attacks. By integrating adversarial attack and de fense methods into model development and deployment, the risk of incorrect Re-ID and ii misclassification can be minimized, leading to robust models. This is especially impor tant in critical applications such as surveillance and security systems. Our thesis aims to propose adversarial attack and defense mechanisms for Re-ID models and certify the robustness of classification models in both white-box and black-box settings. Specifically, we address the limitations of conventional adversaries that consider Euclidean space and ignore the geometry of the pixels. We propose a stronger at tack by incorporating geometry using the Wasserstein metric attack. To defend against such adversarial attacks, we propose a stochastic neural network that uses isotropic and anisotropic Gaussian noise to parameterize stochasticity. These parameters are learned under a meta-learning framework to make our defense more effective and scalable. Finally, in order to provide a provable guarantee of a black-box model robustness, we propose a certified black-box defense via zeroth-order (ZO) optimization for image classification tasks. Previous works suffer from high model variance and low perfor mance on high-dimensional datasets due to inadequate denoiser design and limited uti lization of ZO techniques. To address these limitations, we introduce a robust UNet denoiser (RDUNet). RDUNet enables the model to learn intricate details while main taining low reconstruction error, surpassing the performance of previously developed custom-trained denoisers. We extensively evaluate our proposed generative and adversarial techniques using publicly available Re-ID and classification datasets - Market-1501, DukeMTMC-ReID, MSMT17, CUHK03, Veri-776, CIFAR-10, CIFAR-100, STL-10, Tiny Imagenet, and MNIST.",
                "DOI": "https://repository.iiitd.edu.in/xmlui/handle/123456789/1311",
                "year": "2023-24"
            },
            {
                "title": "Long-Term Ad Memorability: Understanding & Generating Memorable Ads",
                "abstract": "Marketers spend billions of dollars on advertisements, but to what end? At purchase time, if customers cannot recognize the brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no large-scale study on the memorability of ads. All previous memorability studies have been conducted on short-term recall on specific content types like action videos. On the other hand, the advertising industry only cares about long-term memorability, and ads are almost always highly multimodal. Therefore, we release the first memorability dataset, LAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad types, we find many interesting insights into what makes an ad memorable, e.g., fast-moving ads are more memorable than those with slower scenes; people who use ad-blockers remember a lower number of ads than those who don't. Next, we present a model, Henry, to predict the memorability of a content. Henry achieves state-of-the-art performance across all prominent literature memorability datasets. It shows strong generalization performance with better results in 0-shot on unseen datasets. Finally, with the intent of memorable ad generation, we present a scalable method to build a high-quality memorable ad generation model by leveraging automatically annotated data. Our approach, SEED (Self rEwarding mEmorability Modeling), starts with a language model trained on LAMBDA as seed data and progressively trains an LLM to generate more memorable ads. We show that the generated advertisements have 44% higher memorability scores than the original ads. We release this large-scale ad dataset, UltraLAMBDA, consisting of 5 million ads. Our code and datasets are available at https://behavior-in-the-wild.github.io/memorability.",
                "DOI": "https://arxiv.org/abs/2309.00378v4",
                "year": "2023-24"
            },
            {
                "title": "Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning",
                "abstract": "Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the Phoneme Count Compliance (PCC) score, which is a measure of length compliance. Our approach demonstrates a substantial improvement of approximately 36% in the PCC score compared to the state-of-the-art models when applied to English-Hindi language pairs. Moreover, we propose a student-teacher architecture within the framework of our RL approach to maintain a trade-off between the phoneme count and translation quality.",
                "DOI": "https://arxiv.org/abs/2403.15469v1",
                "year": "2023-24"
            },
            {
                "title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot",
                "abstract": "Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification.",
                "DOI": "http://arxiv.org/abs/2305.09758",
                "year": "2023-24"
            },
            {
                "title": "Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021. Exploring Semi-Supervised Learning for Predicting Listener Backchannels. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, Article 395, 1–12. ",
                "DOI": "https://doi.org/10.1145/3411764.3445449",
                "abstract": "Developing human-like conversational agents is a prime area in HCI research and subsumes many tasks. Predicting listener backchannels is one such actively-researched task. While many studies have used different approaches for backchannel prediction, they all have depended on manual annotations for a large dataset. This is a bottleneck impacting the scalability of development. To this end, we propose using semi-supervised techniques to automate the process of identifying backchannels, thereby easing the annotation process. To analyze our identification module’s feasibility, we compared the backchannel prediction models trained on (a) manually-annotated and (b) semi-supervised labels. Quantitative analysis revealed that the proposed semi-supervised approach could attain 95% of the former’s performance. Our user-study findings revealed that almost 60% of the participants found the backchannel responses predicted by the proposed model more natural. Finally, we also analyzed the impact of personality on the type of backchannel signals and validated our findings in the user-study."
            },
            {
                "title": "Srivastava, A., Duan, W., Shah, R. R., Wu, J., Tang, S., Li, W., & Yu, Y. (2022, June). Melody generation from lyrics using three branch conditional LSTM-GAN. In International Conference on Multimedia Modeling (pp. 569-581). Springer, Cham.",
                "DOI": "https://doi.org/10.1007/978-3-030-98358-1_45",
                "abstract": "With the availability of paired lyrics-melody dataset and advancements of artificial intelligence techniques, research on melody generation conditioned on lyrics has become possible. In this work, for melody generation, we propose a novel architecture, Three Branch Conditional (TBC) LSTM-GAN conditioned on lyrics which is composed of a LSTM-based generator and discriminator respectively. The generative model is composed of three branches of identical and independent lyrics-conditioned LSTM-based sub-networks, each responsible for generating an attribute of a melody. For discrete-valued sequence generation, we leverage the Gumbel-Softmax technique to train GANs. Through extensive experiments, we show that our proposed model generates tuneful and plausible melodies from the given lyrics and outperforms the current state-of-the-art models quantitatively as well as qualitatively."
            },
            {
                "title": "Ghosh, S., Kumar, S., Singla, Y. K., Shah, R. R., & Umesh, S. (2022). Span Classification with Structured Information for Disfluency Detection in Spoken Utterances.",
                "DOI": "https://arxiv.org/abs/2203.16028",
                "abstract": "Existing approaches in disfluency detection focus on solving a token-level classification task for identifying and removing disfluencies in text. Moreover, most works focus on leveraging only contextual information captured by the linear sequences in text, thus ignoring the structured information in text which is efficiently captured by dependency trees. In this paper, building on the span classification paradigm of entity recognition, we propose a novel architecture for detecting disfluencies in transcripts from spoken utterances, incorporating both contextual information through transformers and long-distance structured information captured by dependency trees, through graph convolutional networks (GCNs). Experimental results show that our proposed model achieves state-of-the-art results on the widely used English Switchboard for disfluency detection and outperforms prior-art by a significant margin. We make all our codes publicly available on GitHub."
            },
            {
                "title": "Mahata, Debanjan, Naveen Agarwal, Dibya Gautam, Amardeep Kumar, Swapnil Parekh, Yaman Kumar Singla, Anish Acharya, and Rajiv Ratn Shah. \"LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents.\"",
                "DOI": "https://arxiv.org/abs/2203.15349",
                "abstract": "Identifying keyphrases (KPs) from text documents is a fundamental task in natural language processing and information retrieval. Vast majority of the benchmark datasets for this task are from the scientific domain containing only the document title and abstract information. This limits keyphrase extraction (KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from human-written summaries that are often very short (approx 8 sentences). This presents three challenges for real-world applications: human-written summaries are unavailable for most documents, the documents are almost always long, and a high percentage of KPs are directly found beyond the limited context of title and abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M and ~100K scientific articles with their fully extracted text and additional metadata including publication venue, year, author, field of study, and citations for facilitating research on this real-world problem."
            },
            {
                "title": "Bamdev, P., Grover, M. S., Singla, Y. K., Vafaee, P., Hama, M., & Shah, R. R. (2022). Automated Speech Scoring System Under The Lens. International Journal of Artificial Intelligence in Education, 1-36.",
                "DOI": "https://doi.org/10.1007/s40593-022-00291-5",
                "abstract": "English proficiency assessments have become a necessary metric for filtering and selecting prospective candidates for both academia and industry. With the rise in demand for such assessments, it has become increasingly necessary to have the automated human-interpretable results to prevent inconsistencies and ensure meaningful feedback to the second language learners. Feature-based classical approaches have been more interpretable in understanding what the scoring model learns. Therefore, in this work, we utilize classical machine learning models to formulate a speech scoring task as both a classification and a regression problem, followed by a thorough study to interpret and study the relation between the linguistic cues and the English proficiency level of the speaker. First, we extract linguist features under five categories (fuency, pronunciation, content, grammar and vocabulary, and acoustic) and train models to grade responses. In comparison, we find that the regression based models perform equivalent to or better than the classification approach. Second, we perform ablation studies to understand the impact of each of the feature and feature categories on the performance of proficiency grading. Further, to understand individual feature contributions, we present the importance of top features on the best performing algorithm for the grading task. Third, we make use of Partial Dependence Plots and Shapley values to explore feature importance and conclude that the best performing trained model learns the underlying rubrics used for grading the dataset used in this study."
            },
            {
                "title": "Anubha Kabra, Mehar Bhatia, Yaman Kumar Singla, Junyi Jessy Li, and Rajiv Ratn Shah. 2022. Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems. In 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD) (CODSCOMAD 2022), January 8–10, 2022, Bangalore, India. ACM, New York, NY, USA, 10 pages. ",
                "DOI": "https://doi.org/10.1145/3493700.3493765",
                "abstract": "Automatic scoring engines have been used for scoring approximately fifteen million test-takers in just the last three years. This number is increasing further due to COVID-19 and the associated automation of education and testing. Yet, the AI-based testing literature of these ‘intelligent‘ models is highly lacking as most of the papers propose new models that rely only on quadratic weighted kappa (QWK) based agreement with human raters for showing model efficacy. However, this effectively ignores the highly multifeature nature of essay scoring. Essay scoring depends on features like coherence, grammar, relevance, etc. and to date, there has been no study testing Automated Essay Scoring (AES) systems holistically on all these features. With this motivation, we propose a model agnostic adversarial evaluation scheme and associated metrics for AES systems to test their natural language understanding capabilities and overall robustness. We evaluate the current state-of-the-art AES models using the proposed scheme and report the results on five recent models. These models range from feature-engineering based approaches to the latest deep learning algorithms. We find that AES models are highly overstable such that even heavy modifications (as much as 25%) with content unrelated to the topic of the questions do not decrease the score produced by the models. On the other hand, unrelated content, on average, increases the scores, thus showing that the models’ evaluation strategy and rubrics should be reconsidered. We conduct a human survey with 200 human raters and observe that they can easily detect differences between the original and perturbed responses and have a general disagreement with the scores assigned by auto scorers."
            },
            {
                "title": "Shivangi Singhal, Mudit Dhawan, Rajiv R Shah and Ponnurangam Kumaraguru. 2021. Inter-modality Discordance for Multimodal Fake News Detection. In ACM Multimedia Asia (MMAsia ’21), December 1–3, 2021, Gold Coast, Australia. ACM, New York, NY, USA, 7 pages.",
                "DOI": "https://doi.org/10.1145/3469877.3490614",
                "abstract": "The paradigm shift in the consumption of news via online platforms has cultivated the growth of digital journalism. Contrary to traditional media, lowering entry barriers and enabling everyone to be part of content creation have disabled the concept of centralized gatekeeping in digital journalism. This in turn has triggered the production of fake news. Current studies have made a significant effort towards multimodal fake news detection with less emphasis on exploring the discordance between the different multimedia present in a news article. We hypothesize that fabrication of either modality will lead to dissonance between the modalities, and resulting in misrepresented, misinterpreted and misleading news. In this paper, we inspect the authenticity of news coming from online media outlets by exploiting relationship (discordance) between the textual and multiple visual cues. We develop an inter-modality discordance based fake news detection framework to achieve the goal. The modal-specific discriminative features are learned, employing the cross-entropy loss and a modified version of contrastive loss that explores the inter-modality discordance. To the best of our knowledge, this is the first work that leverages information from different components of the news article (i.e., headline, body, and multiple images) for multimodal fake news detection. We conduct extensive experiments on the real-world datasets to show that our approach outperforms the state-of-the-art by an average F1-score of 6.3%."
            },
            {
                "title": "Mohit Sharma, Raj Patra, Harshal Desai, Shruti Vyas, Yogesh Rawat, and Rajiv Ratn Shah. 2021. NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy Labels. In ACM Multimedia Asia (MMAsia ’21), December 1–3, 2021, Gold Coast, Australia. ACM, New York, NY, USA, 5 pages.",
                "DOI": "https://doi.org/10.1145/3469877.3490580",
                "abstract": "Deep learning has shown remarkable progress in a wide range of problems. However, efficient training of such models requires large-scale datasets, and getting annotations for such datasets can be challenging and costly. In this work, we explore user-generated freely available labels from web videos for video understanding. We create a benchmark dataset consisting of around 2 million videos with associated user-generated annotations and other meta information. We utilize the collected dataset for action classification and demonstrate its usefulness with existing small-scale annotated datasets, UCF101 and HMDB51. We study different loss functions and two pretraining strategies, simple and self-supervised learning. We also show how a network pretrained on the proposed dataset can help against video corruption and label noise in downstream datasets. We present this as a benchmark dataset in noisy learning for video understanding. The dataset, code, and trained models are publicly available here for future research. A longer version of our paper is also available here."
            },
            {
                "title": "Bamdev, P., Grover, M. S., Singla, Y. K., Vafaee, P., Hama, M., & Shah, R. R. (2021). Automated Speech Scoring System Under The Lens: Evaluating and interpreting the linguistic cues for language proficiency.",
                "DOI": "https://arxiv.org/pdf/2111.15156.pdf",
                "abstract": "English proficiency assessments have become a necessary metric for filtering and selecting prospective candidates for both academia and industry. With the rise in demand for such assessments, it has become increasingly necessary to have the automated human-interpretable results to prevent inconsistencies and ensure meaningful feedback to the second language learners. Feature-based classical approaches have been more interpretable in understanding what the scoring model learns. Therefore, in this work, we utilize classical machine learning models to formulate a speech scoring task as both a classification and a regression problem, followed by a thorough study to interpret and study the relation between the linguistic cues and the English proficiency level of the speaker. First, we extract linguist features under five categories (fluency, pronunciation, content, grammar and vocabulary, and acoustic) and train models to grade responses. In comparison, we find that the regression-based models perform equivalent to or better than the classification approach. Second, we perform ablation studies to understand the impact of each of the feature and feature categories on the performance of proficiency grading. Further, to understand individual feature contributions, we present the importance of top features on the best performing algorithm for the grading task. Third, we make use of Partial Dependence Plots and Shapley values to explore feature importance and conclude that the best performing trained model learns the underlying rubrics used for grading the dataset used in this study."
            },
            {
                "title": "Singla, Y. K., Krishna, S., Shah, R. R., & Chen, C. (2021). Using Sampling to Estimate and Improve Performance of Automated Scoring Systems with Guarantees.",
                "DOI": "https://arxiv.org/pdf/2111.08906.pdf",
                "abstract": "Automated Scoring (AS), the natural language processing task of scoring essays and speeches in an educational testing setting, is growing in popularity and being deployed across contexts from government examinations to companies providing language proficiency services. However, existing systems either forgo human raters entirely, thus harming the reliability of the test, or score every response by both human and machine thereby increasing costs. We target the spectrum of possible solutions in between, making use of both humans and machines to provide a higher quality test while keeping costs reasonable to democratize access to AS. In this work, we propose a combination of the existing paradigms, sampling responses to be scored by humans intelligently. We propose reward sampling and observe significant gains in accuracy (19.80% increase on average) and quadratic weighted kappa (QWK) (25.60% on average) with a relatively small human budget (30% samples) using our proposed sampling. The accuracy increase observed using standard random and importance sampling baselines are 8.6% and 12.2% respectively. Furthermore, we demonstrate the system's model agnostic nature by measuring its performance on a variety of models currently deployed in an AS setting as well as pseudo models. Finally, we propose an algorithm to estimate the accuracy/QWK with statistical guarantees."
            },
            {
                "title": "A. Verma, A. V. Subramanyam, Z. Wang, S. Satoh and R. R. Shah, \"Unsupervised Domain Adaptation for Person Re-identification via Individual-preserving and Environmental-switching Cyclic Generation,\" in IEEE Transactions on Multimedia.",
                "DOI": "10.1109/TMM.2021.3126404",
                "abstract": "Unsupervised domain adaptation for person re-identification (Re-ID suffers severe domain discrepancies between source and target domains. To reduce the domain shift caused by the changes of context, camera style, or viewpoint, existing methods in this field fine-tune and adapt the Re-ID model with augmented samples, either through translating source samples to the target style or by assigning pseudo labels to the target. The former methods may lose identity details but keep redundant source background during translation, while the latter methods may assign noisy labels when the model meets the unseen background and person pose. To address the challenges, we mitigate the domain shift in the former translation direction by decoupling environment and identity-related features in a cyclic manner. We propose a novel individual-preserving and environmental-switching cyclic generation network (IPES-GAN . Our network has the following distinct features: 1 Decoupled features instead of fused features: the images are encoded into an individual part and an environmental part, which are proved beneficial to generation and adaptation; 2 Cyclic generation instead of one-step adaptive generation. The source and target environment features are swapped to generate cross-domain images with preserved identity-related features conditioned with source (target environment features, and then swapped again to generate back the input image, so that cyclic generation runs in a self-supervised way. Experiments carried out on two major benchmarks: Market-1501 and DukeMTMC-reID reveal state-of-the-art performance."
            },
            {
                "title": "Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, and Rajiv Ratn Shah. 2021. GupShup: Summarizing Open-Domain Code-Switched Conversations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6177–6192, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "DOI": "10.18653/v1/2021.emnlp-main.499",
                "abstract": "Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of Hindi-English (Hi-En) code-switched conversations. We also develop the first code-switched conversation summarization dataset - GupShup, which contains over 6,800 Hi-En conversations and their corresponding human-annotated summaries in English (En) and Hi-En. We present a detailed account of the entire data collection and annotation process. We analyze the dataset using various code-switching statistics. We train state-of-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on this new dataset. We also conduct an extensive qualitative analysis to provide insight into the models and some of their shortcomings."
            },
            {
                "title": "Sharma, D., Kumar, B., Chand, S. et al. A Trend Analysis of Significant Topics Over Time in Machine Learning Research. SN COMPUT. SCI. 2, 469 (2021).",
                "DOI": "https://doi.org/10.1007/s42979-021-00876-2",
                "abstract": "A vast number of research papers on numerous topics publish every year in different conferences and journals. Thus, it is difficult for new researchers to identify research problems and topics manually, which research community is currently focusing on. Since such research problems and topics help researchers to be updated with new topics in research, it is essential to know trends in research based on topic significance over time. Therefore, in this paper, we propose a method to identify the trends in machine learning research based on significant topics over time automatically. Specifically, we apply a topic coherence model with latent Dirichlet allocation (LDA) to evaluate the optimal number of topics and significant topics for a dataset. The LDA model results in topic proportion over documents where each topic has its probability (i.e., topic weight) related to each document. Subsequently, the topic weights are processed to compute average topic weights per year, trend analysis using rolling mean, topic prevalence per year, and topic proportion per journal title. To evaluate our method, we prepare a new dataset comprising of 21,906 scientific research articles from top six journals in the area of machine learning published from 1988 to 2017. Extensive experimental results on the dataset demonstrate that our technique is efficient, and can help upcoming researchers to explore the research trends and topics in different research areas, say machine learning."
            },
            {
                "title": "Parekh, S., Kumar, Y. S., Singh, S., Chen, C., Krishnamurthy, B., & Shah, R. R. (2021). MINIMAL: Mining Models for Data Free Universal Adversarial Triggers.",
                "DOI": "arXiv:2109.12406",
                "abstract": "It is well known that natural language models are vulnerable to adversarial attacks, which are mostly input-specific in nature. Recently, it has been shown that there also exist input-agnostic attacks in NLP models, called universal adversarial triggers. However, existing methods to craft universal triggers are data intensive. They require large amounts of data samples to generate adversarial triggers, which are typically inaccessible by attackers. For instance, previous works take 3000 data samples per class for the SNLI dataset to generate adversarial triggers. In this paper, we present a novel data-free approach, MINIMAL, to mine input-agnostic adversarial triggers from models. Using the triggers produced with our data-free algorithm, we reduce the accuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%. Similarly, for the Stanford Natural Language Inference (SNLI), our single-word trigger reduces the accuracy of the entailment class from 90.95% to less than 0.6%. Despite being completely data-free, we get equivalent accuracy drops as data-dependent methods."
            },
            {
                "title": "Kabra, A., Bhatia, M., Kumar, Y., Li, J. J., Jin, D., & Shah, R. R. (2021). Calling Out Bluff: Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems.",
                "DOI": "arXiv:2007.06796",
                "abstract": "Automatic scoring engines have been used for scoring approximately fifteen million test takers in just the last three years. This number is increasing further due to COVID-19 and the associated automation of education and testing. Despite such wide usage, the AI based testing literature of these ‘intelligent’ models is highly lacking. Most of the papers proposing new models rely only on quadratic weighted kappa (QWK) based agreement with human raters for showing model efficacy. However, this effectively ignores the highly multi-feature nature of essay scoring. Essay scoring depends on features like coherence, grammar, relevance, sufficiency, vocabulary, etc., and till date, there has been no study testing Automated Essay Scoring (AES) systems holistically on all these features. With this motivation, we propose a model agnostic adversarial evaluation scheme and associated metrics for AES systems to test their natural language understanding capabilities and overall robustness. We evaluate the current state-of-the-art AES models using the proposed scheme and report the results on five recent models. These models range from feature-engineering based approaches to the latest deep learning algorithms. We find that AES models are highly overstable such that even heavy modifications (as much as 25%) with content unrelated to the topic of the questions does not decrease the score produced by the models. On the other hand, unrelated content, on average, increases the scores, thus showing that the models’ evaluation strategy and rubrics should be reconsidered. We also ask 200 human raters to score both an original and adversarial response to see if humans are able to detect differences between the two and whether they agree with the scores assigned by autoscorers."
            },
            {
                "title": "Singla, Y. K., Parekh, S., Singh, S., Li, J. J., Shah, R. R., & Chen, C. (2021). AES Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses.",
                "DOI": "arXiv:2109.11728",
                "abstract": "Deep-learning based Automatic Essay Scoring (AES) systems are being actively used by states and language testing agencies alike to evaluate millions of candidates for life-changing decisions ranging from college applications to visa approvals. However, little research has been put to understand and interpret the black-box nature of deep-learning based scoring algorithms. Previous studies indicate that scoring models can be easily fooled. In this paper, we explore the reason behind their surprising adversarial brittleness. We utilize recent advances in interpretability to find the extent to which features such as coherence, content, vocabulary, and relevance are important for automated scoring mechanisms. We use this to investigate the oversensitivity (i.e., large change in output score with a little change in input essay content) and overstability (i.e., little change in output scores with large changes in input essay content) of AES. Our results indicate that auto scoring models, despite getting trained as “end-to-end” models with rich contextual embeddings such as BERT, behave like bag-of-words models. A few words determine the essay score without the requirement of any context making the model largely overstable. This is in stark contrast to recent probing studies on pre-trained representation learning models, which show that rich linguistic features such as parts-of-speech and morphology are encoded by them. Further, we also find that the models have learnt dataset biases, making them oversensitive. The presence of a few words with high co-occurrence with a certain score class makes the model associate the essay sample with that score. This causes score changes in ∼95% of samples with an addition of only a few words. To deal with these issues, we propose detection-based protection models that can detect oversensitivity and overstability causing samples with high accuracies. We find that our proposed models are able to detect unusual attribution patterns and flag adversarial samples successfully."
            },
            {
                "title": "Yaman Kumar Singla, Avyakt Gupta, Shaurya Bagga and Changyou Chen , Balaji Krishnamurthy , Rajiv Ratn Shah . 2021. Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 11 pages.",
                "DOI": "https://doi.org/10.1145/3459637.3482395",
                "abstract": "Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a candidate’s speaking proficiency in a language. ASS systems face many challenges like open grammar, variable pronunciations, and unstructured or semi-structured content. Recent deep learning approaches have shown some promise in this domain. However, most of these approaches focus on extracting features from a single audio, making them suffer from the lack of speaker-specific context required to model such a complex task. We propose a novel deep learning technique for non-native ASS, called speaker-conditioned hierarchical modeling. In our technique, we take advantage of the fact that oral proficiency tests rate multiple responses for a candidate. We extract context vectors from these responses and feed them as additional speaker-specific context to our network to score a particular response. We compare our technique with strong baselines and find that such modeling improves the model’s average performance by 6.92% (maximum = 12.86%, minimum = 4.51%). We further show both quantitative and qualitative insights into the importance of this additional context in solving the problem of ASS."
            },
            {
                "title": "Sawhney, R., Goyal, M., Goel, P., Mathur, P., & Shah, R. (2021, August). Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 6751-6762).",
                "DOI": "10.18653/v1/2021.acl-long.526",
                "abstract": "Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multi-speaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the proposed architecture performing marginally better than strong BERT-based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain."
            },
            {
                "title": "Ramit Sawhney, Shivam Agarwal, Megh Thakkar, Arnav Wadhwa, and Rajiv Ratn Shah. 2021. Hyperbolic Online Time Stream Modeling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 1682–1686.",
                "DOI": "https://doi.org/10.1145/3404835.3463119",
                "abstract": "The rapidly rising ubiquity and dissemination of online information such as social media text and news improve user accessibility towards financial markets, however, modeling these vast streams of irregular, temporal data poses a challenge. Such temporal streams of information show power-law dynamics, scale-free characteristics, and time irregularities that sequential models are unable to accurately model. In this work, we propose the first Hierarchical Time-Aware Hyperbolic LSTM (HTLSTM), which leverages the Riemannian manifold for encoding the scale-free nature of a sequence of text in a time-aware fashion. Through experiments on three financial tasks: stock trading, equity price movement prediction, and financial risk prediction, we demonstrate HTLSTM's applicability for modeling temporal sequences of online information. On real-world data from four global stock markets and three stock indices spanning data in English and Chinese, we make a step towards time-aware text modeling via hyperbolic geometry."
            },
            {
                "title": "Agrawal, M., Mehrotra, P., Kumar, R., & Shah, R. R. (2021). Defending Touch-based Continuous Authentication Systems from Active Adversaries Using Generative Adversarial Networks.",
                "DOI": "https://arxiv.org/abs/2106.07867",
                "abstract": "Previous studies have demonstrated that commonly studied (vanilla) touch-based continuous authentication systems (V-TCAS) are susceptible to population attack. This paper proposes a novel Generative Adversarial Network assisted TCAS (G-TCAS) framework, which showed more resilience to the population attack. G-TCAS framework was tested on a dataset of 117 users who interacted with a smartphone and tablet pair. On average, the increase in the false accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%) for the smartphone. Likewise, the increase in the FARs for V-TCAS was 25% compared to G-TCAS (6%) for the tablet."
            },
            {
                "title": "S. Chopra, P. Mathur, R. Sawhney and R. R. Shah, \"Meta-Learning for Low-Resource Speech Emotion Recognition,\" ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6259-6263.",
                "DOI": "https://ieeexplore.ieee.org/document/9414373",
                "abstract": "While emotion recognition is a well-studied task, it remains unexplored to a large extent in cross-lingual settings. Speech Emotion Recognition (SER) in low-resource languages poses difficulties as existing approaches for knowledge transfer do not generalize seamlessly. Probing the learning process of generalized representations across languages, we propose a meta-learning approach for low-resource speech emotion recognition. The proposed approach achieves fast adaptation on a number of unseen target languages simultaneously. We evaluate the Model Agnostic Meta-Learning (MAML) algorithm on three low-resource target languages -Persian, Italian, and Urdu. We empirically demonstrate that our proposed method - MetaSER 1 , considerably outperforms multitask and transfer learning-based methods for speech emotion recognition task, and discuss the benefits, efficiency, and challenges of MetaSER on limited data settings."
            },
            {
                "title": "A. N. Mathur, D. Batra, Y. K. Singla, R. Ratn Shah, C. Chen and R. Zimmermann, \"LIFI: Towards Linguistically Informed Frame Interpolation,\" ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 7593-7597.",
                "DOI": "https://ieeexplore.ieee.org/document/9413998",
                "abstract": "Here we explore the problem of speech video interpolation. With close to 70% of web traffic, such content today forms the primary form of online communication and entertainment. Despite high performance on conventional metrics like MSE, PSNR, and SSIM, we find that the state-of-the-art frame interpolation models fail to produce faithful speech interpolation. For instance, we observe the lips stay static while the person is still speaking for most interpolated frames. With this motivation, using the information of words, sub-words, and visemes, we provide a new set of linguistically informed metrics targeted explicitly to the problem of speech video interpolation. We release several datasets to test video interpolation models of their speech understanding. We also design linguistically informed deep learning video interpolation algorithms to generate the missing frames."
            },
            {
                "title": "Sawhney, R., Wadhwa, A., Agarwal, S., & Shah, R. (2021, June). Quantitative Day Trading from Natural Language using Reinforcement Learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4018-4030).",
                "DOI": "10.18653/v1/2021.naacl-main.316",
                "abstract": "It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading."
            },
            {
                "title": "Sawhney, R., Aggarwal, A., & Shah, R. (2021, June). An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 3751-3757).",
                "DOI": "10.18653/v1/2021.naacl-main.294",
                "abstract": "Volatility prediction is complex due to the stock market’s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives’ speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender sensitive audio features and fewer female executives in earnings calls of one of the world’s biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes."
            },
            {
                "title": "Sawhney, R., Joshi, H., Shah, R., & Flek, L. (2021, June). Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2176-2190).",
                "DOI": "https://aclanthology.org/2021.naacl-main.176",
                "abstract": "Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user’s emotional history and social information from a user’s neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a time sensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations."
            },
            {
                "title": "Sawhney, R., Mathur, P., Jain, T., Gautam, A. K., & Shah, R. R. Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures.",
                "DOI": "10.18653/v1/2021.naacl-main.387",
                "abstract": "The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better."
            },
            {
                "title": "Sawhney, R., Joshi, H., Nobles, A., & Shah, R. R. (2021). Towards Emotion- and Time-Aware Classification of Tweets to Assist Human Moderation for Suicide Prevention. Proceedings of the International AAAI Conference on Web and Social Media, 15(1), 609-620.",
                "DOI": "https://ojs.aaai.org/index.php/ICWSM/article/view/18088",
                "abstract": "Social media platforms are already engaged in leveraging existing online socio-technical systems to employ just-in-time interventions for suicide prevention to the public. These efforts primarily rely on self-reports of potential self-harm content that is reviewed by moderators. Most recently, platforms have employed automated models to identify self-harm content, but acknowledge that these automated models still struggle to understand the nuance of human language (e.g., sarcasm). By explicitly focusing on Twitter posts that could easily be misidentified by a model as expressing suicidal intent (i.e., they contain similar phrases such as ``wanting to die''), our work examines the temporal differences in historical expressions of general and emotional language prior to a clear expression of suicidal intent. Additionally, we analyze time-aware neural models that build on these language variants and factors in the historical, emotional spectrum of a user's tweeting activity. The strongest model achieves high (statistically significant) performance (macro F1=0.804, recall=0.813) to identify social media indicative of suicidal intent. Using three use cases of tweets with phrases common to suicidal intent, we qualitatively analyze and interpret how such models decided if suicidal intent was present and discuss how these analyses may be used to alleviate the burden on human moderators within the known constraints of how moderation is performed (e.g., no access to the user's timeline). Finally, we discuss the ethical implications of such data-driven models and inferences about suicidal intent from social media. Content warning: this article discusses self-harm and suicide."
            },
            {
                "title": "Sawhney, R., Agarwal, S., Wadhwa, A., Derr, T., & Shah, R. R. (2021, May). Stock Selection via Spatiotemporal Hypergraph Attention Network: A Learning to Rank Approach. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 1, pp. 497-504).",
                "DOI": "https://www.aaai.org/AAAI21Papers/AAAI-7907.SawhneyR.pdf",
                "abstract": "Quantitative trading and investment decision making are intricate financial tasks that rely on accurate stock selection. Despite advances in deep learning that have made significant progress in the complex and highly stochastic stock prediction problem, modern solutions face two significant limitations. They do not directly optimize the target of investment in terms of profit, and treat each stock as independent from the others, ignoring the rich signals between related stocks’ temporal price movements. Building on these limitations, we reformulate stock prediction as a learning to rank problem and propose STHAN-SR, a neural hypergraph architecture for stock selection. The key novelty of our work is the proposal of modeling the complex relations between stocks through a hypergraph and a temporal Hawkes attention mechanism to tailor a new spatiotemporal attention hypergraph network architecture to rank stocks based on profit by jointly modeling stock interdependence and the temporal evolution of their prices. Through experiments on three markets spanning over six years of data, we show that STHAN-SR significantly outperforms state-of-the-art neural stock forecasting methods. We validate our design choices through ablative and exploratory analyses over STHAN-SR’s spatial and temporal components and demonstrate its practical applicability."
            },
            {
                "title": "Yin, Y., Shrivastava, H., Zhang, Y., Liu, Z., Shah, R. R., & Zimmermann, R. (2021, May). Enhanced Audio Tagging via Multi-to Single-Modal Teacher-Student Mutual Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 12, pp. 10709-10717).",
                "DOI": "https://www.aaai.org/AAAI21Papers/AAAI-920.YinY.pdf",
                "abstract": "Recognizing ongoing events based on acoustic clues has been a critical yet challenging problem that has attracted significant research attention in recent years. Joint audio-visual analysis can improve the event detection accuracy but may not always be feasible as under many circumstances only audio recordings are available in real-world scenarios. To solve the challenges, we present a novel visual-assisted teacher student mutual learning framework for robust sound event detection from audio recordings. Our model adopts a multimodal teacher network based on both acoustic and visual clues, and a single-modal student network based on acoustic clues only. Conventional teacher-student learning performs unsatisfactorily for knowledge transfer from a multi-modality network to a single-modality network. We thus present a mutual learning framework by introducing a single-modal transfer loss and a cross-modal transfer loss to collaboratively learn the audio-visual correlations between the two networks. Our proposed solution takes the advantages of joint audiovisual analysis in training while maximizing the feasibility of the model in use cases. Our extensive experiments on the DCASE17 and the DCASE18 sound event detection datasets show that our proposed method outperforms the state-of-the art audio tagging approaches."
            },
            {
                "title": "Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021. Exploring Semi-Supervised Learning for Predicting Listener Backchannels. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 12 pages.",
                "DOI": "https://doi.org/10.1145/3411764.3445449",
                "abstract": "Developing human-like conversational agents is a prime area in HCI research and subsumes many tasks. Predicting listener backchannels is one such actively-researched task. While many studies have used different approaches for backchannel prediction, they all have depended on manual annotations for a large dataset. This is a bottleneck impacting the scalability of development. To this end, we propose using semi-supervised techniques to automate the process of identifying backchannels, thereby easing the annotation process. To analyze our identification module’s feasibility, we compared the backchannel prediction models trained on (a) manually-annotated and (b) semi-supervised labels. Quantitative analysis revealed that the proposed semi-supervised approach could attain 95% of the former’s performance. Our user-study fndings revealed that almost 60% of the participants found the backchannel responses predicted by the proposed model more natural. Finally, we also analyzed the impact of personality on the type of backchannel signals and validated our fndings in the user-study."
            },
            {
                "title": "Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Ratn Shah. 2021. Exploring the Scale-Free Nature of Stock Markets: Hyperbolic Graph Learning for Algorithmic Trading. In Proceedings of the Web Conference 2021 (WWW ’21), April 19–23, 2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 12 pages.",
                "DOI": "https://doi.org/10.1145/3442381.3450095",
                "abstract": "Quantitative trading and investment decision making are intricate financial tasks in the ever-increasing sixty trillion dollars global stock market. Despite advances in stock forecasting, a limitation of most existing neural methods is that they treat stocks independent of each other, ignoring the valuable rich signals between related stocks’ movements. Motivated by financial literature that shows stock markets and inter-stock correlations show scale-free network characteristics, we leverage domain knowledge on the Web to model inter-stock relations as a graph in four major global stock markets and formulate stock selection as a scale-free graph-based learning to rank problem. To capture the scale-free spatial and temporal dependencies in stock prices, we propose HyperStockGAT: Hyperbolic Stock Graph Attention Network, the first model on the Riemannian Manifolds for stock selection. Our work’s key novelty is the proposal of modeling the complex, scale-free nature of inter-stock relations through temporal hyperbolic graph learning on Riemannian manifolds that can represent the spatial correlations between stocks more accurately. Through extensive experiments on long-term real-world data spanning over six years on four of the world’s biggest markets: NASDAQ, NYSE, TSE, and China exchanges, we show that HyperStockGAT significantly outperforms state-of-the-art stock forecasting methods in terms of profitability by over 12%, and risk-adjusted Sharpe Ratio by over 4%. We analyze HyperStockGAT’s components’ contributions through a series of exploratory and ablative experiments to demonstrate its practical applicability to real-world trading. Furthermore, we propose a novel hyperbolic architecture that can be applied across various spatiotemporal problems on the Web’s commonly occurring scale-free networks."
            },
            {
                "title": "Sawhney, R., Joshi, H., Gandhi, S., & Shah, R. R. (2021, March). Towards Ordinal Suicide Ideation Detection on Social Media. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (pp. 22-30).",
                "DOI": "https://doi.org/10.1145/3437963.3441805",
                "abstract": "The rising ubiquity of social media presents a platform for individuals to express suicide ideation, instead of traditional, formal clinical settings. While neural methods for assessing suicide risk on social media have shown promise, a crippling limitation of existing solutions is that they ignore the inherent ordinal nature across finegrain levels of suicide risk. To this end, we reformulate suicide risk assessment as an Ordinal Regression problem, over the ColumbiaSuicide Severity Scale. We propose SISMO, a hierarchical attention model optimized to factor in the graded nature of increasing suicide risk levels, through soft probability distribution since not all wrong risk-levels are equally wrong. We establish the face value of SISMO for preliminary suicide risk assessment on real-world Reddit data annotated by clinical experts. We conclude by discussing the empirical, practical, and ethical considerations pertaining to SISMO in a larger picture, as a human-in-the-loop framework."
            },
            {
                "title": "Mehnaz, L., Mahata, D., Gosangi, R., Gunturi, U. S., Jain, R., Gupta, G., ... & Shah, R. R. (2021). GupShup: An Annotated Corpus for Abstractive Summarization of Open-Domain Code-Switched Conversations.",
                "DOI": "https://arxiv.org/abs/2104.08578",
                "abstract": "Code-switching is the communication phenomenon where speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. This makes it essential to develop techniques for summarizing and understanding these conversations. Towards this objective, we introduce abstractive summarization of Hindi-English code-switched conversations and develop the first code-switched conversation summarization dataset - GupShup, which contains over 6,831 conversations in Hindi-English and their corresponding human annotated summaries in English and Hindi-English. We present a detailed account of the entire data collection and annotation processes. We analyze the dataset using various code-switching statistics. We train stateof-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on the new dataset."
            },
            {
                "title": "Sawhney, R., Wadhwa, A., Agarwal, S., & Shah, R. (2021, April). FAST: Financial News and Tweet Based Time Aware Network for Stock Trading. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (pp. 2164-2175).",
                "DOI": "https://aclanthology.org/2021.eacl-main.185.pdf",
                "abstract": "Designing profitable trading strategies is complex as stock movements are highly stochastic; the market is influenced by large volumes of noisy data across diverse information sources like news and social media. Prior work mostly treats stock movement prediction as a regression or classification task and is not directly optimized towards profit-making. Further, they do not model the fine-grain temporal irregularities in the release of vast volumes of text that the market responds to quickly. Building on these limitations, we propose a novel hierarchical, learning to rank approach that uses textual data to make time-aware predictions for ranking stocks based on expected profit. Our approach outperforms state-of-the-art methods by over 8% in terms of cumulative profit and risk-adjusted returns in trading simulations on two benchmarks: English tweets and Chinese financial news spanning two major stock indexes and four global markets. Through ablative and qualitative analyses, we build the case for our method as a tool for daily stock trading. "
            },
            {
                "title": "Kashyap, A. R., Mehnaz, L., Malik, B., Waheed, A., Hazarika, D., Kan, M. Y., & Shah, R. (2021, April). Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer. In Proceedings of the Second Workshop on Domain Adaptation for NLP (pp. 222-244).",
                "DOI": "https://aclanthology.org/2021.adaptnlp-1.23/",
                "abstract": "The robustness of pretrained language models(PLMs) is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i) Domain variance increases from the lower to the upper layers for vanilla PLMs; ii) Models continuously pretrained on domain-specific data (DAPT)(Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g., DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain."
            },
            {
                "title": "Farooqi, Z. M., Ghosh, S., & Shah, R. R. (2021). Leveraging Transformers for Hate Speech Detection in Conversational Code-Mixed Tweets.",
                "DOI": "arXiv:2112.09986",
                "abstract": "In the current era of the internet, where social media platforms are easily accessible for everyone, people often have to deal with threats, identity attacks, hate, and bullying due to their association with a cast, creed, gender, religion, or even acceptance or rejection of a notion. Existing works in hate speech detection primarily focus on individual comment classification as a sequence labeling task and often fail to consider the context of the conversation. The context of a conversation often plays a substantial role when determining the author's intent and sentiment behind the tweet. This paper describes the system proposed by team MIDAS-IIITD for HASOC 2021 subtask 2, one of the first shared tasks focusing on detecting hate speech from Hindi-English code-mixed conversations on Twitter. We approach this problem using neural networks, leveraging the transformer's cross-lingual embeddings and further finetuning them for low-resource hate-speech classification in transliterated Hindi text. Our best performing system, a hard voting ensemble of Indic-BERT, XLM-RoBERTa, and Multilingual BERT, achieved a macro F1 score of 0.7253, placing us first on the overall leaderboard standings."
            },
            {
                "title": "Bamdev, P., Grover, M. S., Singla, Y. K., Vafaee, P., Hama, M., & Shah, R. R. (2021). Automated Speech Scoring System Under The Lens: Evaluating and interpreting the linguistic cues for language proficiency.",
                "DOI": "https://arxiv.org/abs/2111.15156",
                "abstract": "English proficiency assessments have become a necessary metric for filtering and selecting prospective candidates for both academia and industry. With the rise in demand for such assessments, it has become increasingly necessary to have the automated human-interpretable results to prevent inconsistencies and ensure meaningful feedback to the second language learners. Feature-based classical approaches have been more interpretable in understanding what the scoring model learns. Therefore, in this work, we utilize classical machine learning models to formulate a speech scoring task as both a classification and a regression problem, followed by a thorough study to interpret and study the relation between the linguistic cues and the English proficiency level of the speaker. First, we extract linguist features under five categories (fluency, pronunciation, content, grammar and vocabulary, and acoustic) and train models to grade responses. In comparison, we find that the regression-based models perform equivalent to or better than the classification approach. Second, we perform ablation studies to understand the impact of each of the feature and feature categories on the performance of proficiency grading. Further, to understand individual feature contributions, we present the importance of top features on the best performing algorithm for the grading task. Third, we make use of Partial Dependence Plots and Shapley values to explore feature importance and conclude that the best performing trained model learns the underlying rubrics used for grading the dataset used in this study."
            },
            {
                "title": "Singla, Y. K., Krishna, S., Shah, R. R., & Chen, C. (2021). Using Sampling to Estimate and Improve Performance of Automated Scoring Systems with Guarantees.",
                "DOI": "https://arxiv.org/pdf/2111.08906.pdf",
                "abstract": "Automated Scoring (AS), the natural language processing task of scoring essays and speeches in an educational testing setting, is growing in popularity and being deployed across contexts from government examinations to companies providing language proficiency services. However, existing systems either forgo human raters entirely, thus harming the reliability of the test, or score every response by both human and machine thereby increasing costs. We target the spectrum of possible solutions in between, making use of both humans and machines to provide a higher quality test while keeping costs reasonable to democratize access to AS. In this work, we propose a combination of the existing paradigms, sampling responses to be scored by humans intelligently. We propose reward sampling and observe significant gains in accuracy (19.80% increase on average) and quadratic weighted kappa (QWK) (25.60% on average) with a relatively small human budget (30% samples) using our proposed sampling. The accuracy increase observed using standard random and importance sampling baselines are 8.6% and 12.2% respectively. Furthermore, we demonstrate the system's model agnostic nature by measuring its performance on a variety of models currently deployed in an AS setting as well as pseudo models. Finally, we propose an algorithm to estimate the accuracy/QWK with statistical guarantee."
            },
            {
                "title": "A. Verma, A. V. Subramanyam, Z. Wang, S. Satoh and R. R. Shah, \"Unsupervised Domain Adaptation for Person Re-identification via Individual-preserving and Environmental-switching Cyclic Generation,\" in IEEE Transactions on Multimedia.",
                "DOI": "10.1109/TMM.2021.3126404",
                "abstract": "Unsupervised domain adaptation for person re-identification (Re-ID suffers severe domain discrepancies between source and target domains. To reduce the domain shift caused by the changes of context, camera style, or viewpoint, existing methods in this field fine-tune and adapt the Re-ID model with augmented samples, either through translating source samples to the target style or by assigning pseudo labels to the target. The former methods may lose identity details but keep redundant source background during translation, while the latter methods may assign noisy labels when the model meets the unseen background and person pose. To address the challenges, we mitigate the domain shift in the former translation direction by decoupling environment and identity-related features in a cyclic manner. We propose a novel individual-preserving and environmental-switching cyclic generation network (IPES-GAN . Our network has the following distinct features: 1 Decoupled features instead of fused features: the images are encoded into an individual part and an environmental part, which are proved beneficial to generation and adaptation; 2 Cyclic generation instead of one-step adaptive generation. The source and target environment features are swapped to generate cross-domain images with preserved identity-related features conditioned with source (target environment features, and then swapped again to generate back the input image, so that cyclic generation runs in a self-supervised way. Experiments carried out on two major benchmarks: Market-1501 and DukeMTMC-reID reveal state-of-the-art performance."
            },
            {
                "title": "Mehnaz, L., Mahata, D., Gosangi, R., Gunturi, U.S., Jain, R., Gupta, G., Kumar, A., Lee, I.G., Acharya, A., & Shah, R.R. (2021). GupShup: Summarizing Open-Domain Code-Switched Conversations. EMNLP.",
                "DOI": "https://aclanthology.org/2021.emnlp-main.499",
                "abstract": "Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of Hindi-English (Hi-En) code-switched conversations. We also develop the first code-switched conversation summarization dataset - GupShup, which contains over 6,800 Hi-En conversations and their corresponding human-annotated summaries in English (En) and Hi-En. We present a detailed account of the entire data collection and annotation process. We analyze the dataset using various code-switching statistics. We train state-of-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on this new dataset. We also conduct an extensive qualitative analysis to provide insight into the models and some of their shortcomings."
            }
        ]
    },
    {
        "profInfo": {
            "image": "richagupta.png",
            "name": "Dr. Richa Gupta",
            "archive" : false
        },
        "projects": [
            {
                "title": "StoryBox: Independent Multi-modal Interactive Storytelling for Children with Visual Impairment",
                "DOI": "https://doi.org/10.1145/3491101.3519651",
                "abstract": "Storytelling promotes the development of imagination and creativity and facilitates the impregnation of linguistic, emotional and moral skills in children. Limited storytelling media for children with blindness and visual impairment (BVI) confines their access to this unique experience. StoryBox is a multi-modal storytelling agent, facilitating an independent and interactive storytelling experience for children (5-12 years) with BVI. The platform supports audio-based story narration and enables tangible interaction with story elements via sensor-enabled tactile clay figurines. StoryBox takes inspiration from ‘kaavad baanchana’ - an oral storytelling tradition from Rajasthan, India. This paper presents the design of the proposed accessible multi-modal storytelling platform and findings from initial user evaluation with blindfolded participants.",
                "year": "2022-23"
            },
            {
                "title": "\"Perceiving Sequences and Layouts through Touch\" has been accepted at IEEE Eurohaptics Conference 2022 ",
                "DOI": "N/A",
                "abstract": "Effective design of tactile graphics demands an in-depth investigation of perceptual foundations of exploration through touch. This work investigates primitives in tactile perception of spatial arrangements (i.e. sequences and layouts). Two experiments using tiles with different tactile shapes were arranged in a row on a tabletop or within a 5x5 grid board. The goal of the experiments was to determine whether certain positions offered perceptual salience. The results indicate that positional primitives exist (e.g. corners, field edges and first and last positions in sequences), and these reinforce memory of spatial relationships. These inferences can influence effective tactile graphic design as well as design of inclusive and multi-modal interfaces/experiences."
            }
        ]
    },
    {
        "profInfo": {
            "image": "sonal.jpg",
            "name": "Dr. Sonal Keshwani",
            "archive" : false
        },
        "projects": [
        ]
    }
]